{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7fed5f",
   "metadata": {
    "id": "9a7fed5f"
   },
   "source": [
    "# Week 41: Model Evaluation on Custom Test Set\n",
    "\n",
    "\n",
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb270dc",
   "metadata": {
    "id": "9eb270dc"
   },
   "source": [
    "### 1.1 Environment Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31f40fb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1421,
     "status": "ok",
     "timestamp": 1761434110902,
     "user": {
      "displayName": "Themis Ch",
      "userId": "10107086598836666206"
     },
     "user_tz": -120
    },
    "id": "31f40fb9",
    "outputId": "dfa44832-9249-4b4f-b8a6-99ac543ec591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Google Colab\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Environment detection\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"Running in local environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828d25c",
   "metadata": {
    "id": "d828d25c"
   },
   "source": [
    "### 1.2 Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddabf611",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10677,
     "status": "ok",
     "timestamp": 1761434121580,
     "user": {
      "displayName": "Themis Ch",
      "userId": "10107086598836666206"
     },
     "user_tz": -120
    },
    "id": "ddabf611",
    "outputId": "38ec68f5-2f0e-4a99-adab-980d47768523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from seqeval) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba13048",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1761434121890,
     "user": {
      "displayName": "Themis Ch",
      "userId": "10107086598836666206"
     },
     "user_tz": -120
    },
    "id": "7ba13048",
    "outputId": "88ce2142-bbbc-4c41-a2f4-57bc01dddb9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch device: cpu\n",
      "âœ“ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Core Python libraries\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning and NLP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "# Sklearn metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Sequence evaluation\n",
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "from seqeval.metrics import f1_score as seqeval_f1\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# PyTorch setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup stopwords for English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(\"All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822266e6",
   "metadata": {
    "id": "822266e6"
   },
   "source": [
    "### 1.3 Load Custom Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518751f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 169,
     "status": "ok",
     "timestamp": 1761434122076,
     "user": {
      "displayName": "Themis Ch",
      "userId": "10107086598836666206"
     },
     "user_tz": -120
    },
    "id": "518751f2",
    "outputId": "37dfcb81-48c2-4c67-f778-23721ca42ac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from: /content/drive/MyDrive/Colab_Notebooks/NLP/test.json\n",
      "\n",
      "âœ“ Loaded 30 test questions\n",
      "\n",
      "Language distribution:\n",
      "lang\n",
      "en    24\n",
      "el     6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Answerability distribution:\n",
      "answerable\n",
      "True     22\n",
      "False     8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "======================================================================\n",
      "SAMPLE TEST QUESTIONS\n",
      "======================================================================\n",
      "\n",
      "Question 1 (en):\n",
      "  Q: Who wrote the novel '1984'?\n",
      "  A: George Orwell\n",
      "  Context: 1984 is a dystopian novel published in 1949 by English writer George Orwell. It depicts a totalitari...\n",
      "\n",
      "Question 2 (en):\n",
      "  Q: What is the capital city of Japan?\n",
      "  A: Tokyo\n",
      "  Context: Japan is an island country in East Asia. Its capital and largest city is Tokyo, known for its modern...\n",
      "\n",
      "Question 3 (en):\n",
      "  Q: When did World War II end?\n",
      "  A: 1945\n",
      "  Context: World War II was a global conflict that lasted from 1939 to 1945. It ended with the unconditional su...\n"
     ]
    }
   ],
   "source": [
    "# Load custom test set\n",
    "if IN_COLAB:\n",
    "    TEST_FILE = \"/content/drive/MyDrive/Colab_Notebooks/NLP/test.json\"\n",
    "else:\n",
    "    TEST_FILE = \"./questions.json\"\n",
    "\n",
    "print(f\"Loading test data from: {TEST_FILE}\")\n",
    "\n",
    "with open(TEST_FILE, 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "print(f\"\\nLoaded {len(df_test)} test questions\")\n",
    "print(f\"\\nLanguage distribution:\")\n",
    "print(df_test['lang'].value_counts())\n",
    "print(f\"\\nAnswerability distribution:\")\n",
    "print(df_test['answerable'].value_counts())\n",
    "\n",
    "# Display sample questions\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE TEST QUESTIONS\")\n",
    "print(\"=\"*70)\n",
    "for i in range(min(3, len(df_test))):\n",
    "    row = df_test.iloc[i]\n",
    "    print(f\"\\nQuestion {i+1} ({row['lang']}):\")\n",
    "    print(f\"  Q: {row['question']}\")\n",
    "    print(f\"  A: {row['answer'] if row['answerable'] else 'Not answerable'}\")\n",
    "    print(f\"  Context: {row['context'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ccd73",
   "metadata": {
    "id": "d03ccd73"
   },
   "source": [
    "---\n",
    "\n",
    "## 2. Week 36: Rule-Based Answerability Classifier\n",
    "\n",
    "Using TF-IDF weighted keyword overlap to classify questions as answerable or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a43fe",
   "metadata": {
    "id": "a73a43fe"
   },
   "source": [
    "### 2.1 Load Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880dff10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37232,
     "status": "ok",
     "timestamp": 1761434159326,
     "user": {
      "displayName": "Themis Ch",
      "userId": "10107086598836666206"
     },
     "user_tz": -120
    },
    "id": "880dff10",
    "outputId": "b35376a9-e0c9-4862-b0e6-9e20af1e7e4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NLLB translation model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Translation model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize translation model for multilingual questions\n",
    "device_id = 0 if torch.cuda.is_available() else -1\n",
    "TARGET_LANG = 'eng_Latn'\n",
    "\n",
    "print(\"Loading NLLB translation model...\")\n",
    "translator = pipeline(\n",
    "    \"translation\",\n",
    "    model=\"facebook/nllb-200-distilled-600M\",\n",
    "    device=device_id,\n",
    "    torch_dtype=torch.float16 if device_id == 0 else torch.float32\n",
    ")\n",
    "\n",
    "print(\"Translation model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea70e36",
   "metadata": {
    "id": "dea70e36"
   },
   "source": [
    "### 2.2 Define Rule-Based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "802f0430",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1761434159372,
     "user": {
      "displayName": "Themis Ch",
      "userId": "10107086598836666206"
     },
     "user_tz": -120
    },
    "id": "802f0430",
    "outputId": "8693cac7-8b4e-49d0-86ff-3f78d1967ed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Rule-based classifier defined\n"
     ]
    }
   ],
   "source": [
    "# Language code mapping for NLLB\n",
    "NLLB_LANG_CODES = {\n",
    "    'ar': 'arb_Arab',\n",
    "    'ko': 'kor_Hang',\n",
    "    'te': 'tel_Telu',\n",
    "    'en': 'eng_Latn',\n",
    "    'el': 'ell_Grek',\n",
    "}\n",
    "\n",
    "class CrossLingualAnswerabilityClassifier:\n",
    "    \"\"\"Rule-based classifier using TF-IDF weighted keyword overlap.\"\"\"\n",
    "\n",
    "    def __init__(self, threshold=0.1, min_keyword_length=3):\n",
    "        self.threshold = threshold\n",
    "        self.min_keyword_length = min_keyword_length\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
    "\n",
    "    def extract_keywords(self, question_translated):\n",
    "        \"\"\"Extract meaningful keywords from translated question.\"\"\"\n",
    "        words = nltk.word_tokenize(question_translated.lower())\n",
    "        keywords = [\n",
    "            word for word in words\n",
    "            if (word.isalnum() and\n",
    "                len(word) > self.min_keyword_length and\n",
    "                word not in stop_words)\n",
    "        ]\n",
    "        return keywords\n",
    "\n",
    "    def compute_overlap_score(self, keywords, context):\n",
    "        \"\"\"Compute TF-IDF weighted overlap score.\"\"\"\n",
    "        if not keywords:\n",
    "            return 0.0\n",
    "\n",
    "        question_text = ' '.join(keywords)\n",
    "        documents = [question_text, context.lower()]\n",
    "\n",
    "        try:\n",
    "            tfidf_matrix = self.tfidf_vectorizer.fit_transform(documents)\n",
    "            vocab = self.tfidf_vectorizer.vocabulary_\n",
    "\n",
    "            overlap_score = 0.0\n",
    "            context_words = set(nltk.word_tokenize(context.lower()))\n",
    "\n",
    "            for keyword in keywords:\n",
    "                if keyword in vocab and keyword in context_words:\n",
    "                    keyword_idx = vocab[keyword]\n",
    "                    tfidf_score = tfidf_matrix[0, keyword_idx]\n",
    "                    overlap_score += tfidf_score\n",
    "\n",
    "            return overlap_score\n",
    "\n",
    "        except Exception:\n",
    "            # Fallback: simple overlap\n",
    "            context_words = set(nltk.word_tokenize(context.lower()))\n",
    "            simple_overlap = sum(1 for kw in keywords if kw in context_words)\n",
    "            return simple_overlap / len(keywords) if keywords else 0.0\n",
    "\n",
    "    def predict_single(self, question, context, language_code):\n",
    "        \"\"\"Predict answerability for a single question-context pair.\"\"\"\n",
    "        try:\n",
    "            # Translate question if needed\n",
    "            if language_code == 'en':\n",
    "                translated_q = question\n",
    "                translated_ctx = context\n",
    "            else:\n",
    "                src = NLLB_LANG_CODES.get(language_code, 'eng_Latn')\n",
    "                translation = translator(question, src_lang=src, tgt_lang=TARGET_LANG)\n",
    "                translated_q = translation[0]['translation_text'] if isinstance(translation, list) else translation['translation_text']\n",
    "                tc = translator(context, src_lang=src, tgt_lang=TARGET_LANG)\n",
    "                translated_ctx = tc[0]['translation_text'] if isinstance(tc, list) else tc['translation_text']\n",
    "\n",
    "            # Extract keywords and compute score\n",
    "            keywords = self.extract_keywords(translated)\n",
    "            score = self.compute_overlap_score(keywords, context)\n",
    "            prediction = score > self.threshold\n",
    "\n",
    "            return prediction, score, translated, keywords\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: prediction failed - {e}\")\n",
    "            return False, 0.0, question, []\n",
    "\n",
    "print(\"âœ“ Rule-based classifier defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ec01bd",
   "metadata": {
    "id": "a3ec01bd"
   },
   "source": [
    "### 2.3 Evaluate W36 Classifier on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8d54c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 183857,
     "status": "ok",
     "timestamp": 1761434343256,
     "user": {
      "displayName": "Themis Ch",
      "userId": "10107086598836666206"
     },
     "user_tz": -120
    },
    "id": "79b8d54c",
    "outputId": "2dce9faa-a98d-4355-90f0-12e32c72796f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "WEEK 36: RULE-BASED ANSWERABILITY CLASSIFIER\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š Results on 30 test questions:\n",
      "  Accuracy:  0.3000\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.0455\n",
      "  F1-score:  0.0870\n",
      "\n",
      "Confusion Matrix:\n",
      "  TN:   8  FP:   0\n",
      "  FN:  21  TP:   1\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE PREDICTIONS\n",
      "======================================================================\n",
      "\n",
      "Example 1 âœ—:\n",
      "  Question: Who wrote the novel '1984'?...\n",
      "  Language: en\n",
      "  Predicted: Not answerable\n",
      "  True:      Answerable\n",
      "\n",
      "Example 2 âœ—:\n",
      "  Question: What is the capital city of Japan?...\n",
      "  Language: en\n",
      "  Predicted: Not answerable\n",
      "  True:      Answerable\n",
      "\n",
      "Example 3 âœ—:\n",
      "  Question: When did World War II end?...\n",
      "  Language: en\n",
      "  Predicted: Not answerable\n",
      "  True:      Answerable\n",
      "\n",
      "Example 4 âœ—:\n",
      "  Question: Which planet is known as the Red Planet?...\n",
      "  Language: en\n",
      "  Predicted: Not answerable\n",
      "  True:      Answerable\n",
      "\n",
      "Example 5 âœ—:\n",
      "  Question: Who painted the Mona Lisa?...\n",
      "  Language: en\n",
      "  Predicted: Not answerable\n",
      "  True:      Answerable\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEEK 36: RULE-BASED ANSWERABILITY CLASSIFIER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize classifier\n",
    "w36_classifier = CrossLingualAnswerabilityClassifier(threshold=0.1)\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for idx, row in df_test.iterrows():\n",
    "    pred, score, translated, keywords = w36_classifier.predict_single(\n",
    "        row['question'],\n",
    "        row['context'],\n",
    "        row['lang']\n",
    "    )\n",
    "    predictions.append(pred)\n",
    "    true_labels.append(row['answerable'])\n",
    "\n",
    "# Calculate metrics\n",
    "w36_accuracy = accuracy_score(true_labels, predictions)\n",
    "w36_precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "w36_recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "w36_f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\n Results on {len(df_test)} test questions:\")\n",
    "print(f\"  Accuracy:  {w36_accuracy:.4f}\")\n",
    "print(f\"  Precision: {w36_precision:.4f}\")\n",
    "print(f\"  Recall:    {w36_recall:.4f}\")\n",
    "print(f\"  F1-score:  {w36_f1:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TN: {cm[0,0]:3d}  FP: {cm[0,1]:3d}\")\n",
    "print(f\"  FN: {cm[1,0]:3d}  TP: {cm[1,1]:3d}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "for i in range(min(5, len(df_test))):\n",
    "    row = df_test.iloc[i]\n",
    "    pred = predictions[i]\n",
    "    true = true_labels[i]\n",
    "    correct = \"âœ“\" if pred == true else \"âœ—\"\n",
    "\n",
    "    print(f\"\\nExample {i+1} {correct}:\")\n",
    "    print(f\"  Question: {row['question'][:80]}...\")\n",
    "    print(f\"  Language: {row['lang']}\")\n",
    "    print(f\"  Predicted: {'Answerable' if pred else 'Not answerable'}\")\n",
    "    print(f\"  True:      {'Answerable' if true else 'Not answerable'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed4514",
   "metadata": {
    "id": "c8ed4514"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Week 38: Trained Answerability Classifier (XLM-RoBERTa)\n",
    "\n",
    "Using a fine-tuned XLM-RoBERTa model for multilingual answerability classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfcfb6",
   "metadata": {
    "id": "25cfcfb6"
   },
   "source": [
    "### 3.1 Load Pre-trained Model\n",
    "\n",
    "Note: This assumes the model was previously trained and saved. If not available, this section will train a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "be49896a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9133,
     "status": "ok",
     "timestamp": 1761434352393,
     "user": {
      "displayName": "Themis Ch",
      "userId": "10107086598836666206"
     },
     "user_tz": -120
    },
    "id": "be49896a",
    "outputId": "0e9b9a21-3aae-43fd-9b65-d9bae79c1560"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.12/dist-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacremoses) (2024.11.6)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses) (8.3.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses) (1.5.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sacremoses) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81b7f6a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8924,
     "status": "ok",
     "timestamp": 1761434361319,
     "user": {
      "displayName": "Themis Ch",
      "userId": "10107086598836666206"
     },
     "user_tz": -120
    },
    "id": "81b7f6a7",
    "outputId": "7fee99c2-6a56-424f-e21b-8f35a50bf6d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained model from /content/drive/MyDrive/Colab_Notebooks/NLP/tydi_xor_rc/models/xlmr_answerability_ar...\n",
      "âœ“ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MAX_CONTEXT_CHARS = 1500\n",
    "MAX_SEQ_LEN_TRANSFORMER = 512\n",
    "\n",
    "# Model path\n",
    "if IN_COLAB:\n",
    "    W38_MODEL_PATH = \"/content/drive/MyDrive/Colab_Notebooks/NLP/tydi_xor_rc/models/xlmr_answerability_ar\"\n",
    "else:\n",
    "    W38_MODEL_PATH = \"./models/xlmr_answerability\"\n",
    "\n",
    "# Try to load saved model\n",
    "if os.path.exists(W38_MODEL_PATH):\n",
    "    print(f\"Loading pre-trained model from {W38_MODEL_PATH}...\")\n",
    "    w38_tokenizer = AutoTokenizer.from_pretrained(W38_MODEL_PATH)\n",
    "    w38_model = AutoModelForSequenceClassification.from_pretrained(W38_MODEL_PATH)\n",
    "    w38_model.to(device)\n",
    "    w38_model.eval()\n",
    "    print(\"âœ“ Model loaded successfully\")\n",
    "else:\n",
    "    print(f\"âš  Model not found at {W38_MODEL_PATH}\")\n",
    "    print(\"  Using base XLM-R model (not fine-tuned)\")\n",
    "    w38_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    w38_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"xlm-roberta-base\",\n",
    "        num_labels=2\n",
    "    )\n",
    "    w38_model.to(device)\n",
    "    w38_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fcc92d",
   "metadata": {
    "id": "b7fcc92d"
   },
   "source": [
    "### 3.2 Evaluate W38 Classifier on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d0249c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21897,
     "status": "ok",
     "timestamp": 1761434383220,
     "user": {
      "displayName": "Themis Ch",
      "userId": "10107086598836666206"
     },
     "user_tz": -120
    },
    "id": "68d0249c",
    "outputId": "df1afa60-5bad-486c-fe0e-8803141150b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "WEEK 38: XLM-RoBERTa ANSWERABILITY CLASSIFIER\n",
      "======================================================================\n",
      "Making predictions on 30 test examples...\n",
      "\n",
      "ðŸ“Š Results on 30 test questions:\n",
      "  Accuracy:  0.4667\n",
      "  Precision: 1.0000\n",
      "  Recall:    0.2727\n",
      "  F1-score:  0.4286\n",
      "\n",
      "Confusion Matrix:\n",
      "  TN:   8  FP:   0\n",
      "  FN:  16  TP:   6\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE PREDICTIONS\n",
      "======================================================================\n",
      "\n",
      "Example 1 âœ“:\n",
      "  Question: Who wrote the novel '1984'?...\n",
      "  Language: en\n",
      "  Predicted: Answerable\n",
      "  True:      Answerable\n",
      "\n",
      "Example 2 âœ—:\n",
      "  Question: What is the capital city of Japan?...\n",
      "  Language: en\n",
      "  Predicted: Not answerable\n",
      "  True:      Answerable\n",
      "\n",
      "Example 3 âœ—:\n",
      "  Question: When did World War II end?...\n",
      "  Language: en\n",
      "  Predicted: Not answerable\n",
      "  True:      Answerable\n",
      "\n",
      "Example 4 âœ—:\n",
      "  Question: Which planet is known as the Red Planet?...\n",
      "  Language: en\n",
      "  Predicted: Not answerable\n",
      "  True:      Answerable\n",
      "\n",
      "Example 5 âœ“:\n",
      "  Question: Who painted the Mona Lisa?...\n",
      "  Language: en\n",
      "  Predicted: Answerable\n",
      "  True:      Answerable\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEEK 38: XLM-RoBERTa ANSWERABILITY CLASSIFIER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare test data\n",
    "df_test_copy = df_test.copy()\n",
    "df_test_copy[\"context_trunc\"] = df_test_copy[\"context\"].str[:MAX_CONTEXT_CHARS]\n",
    "df_test_copy[\"text\"] = df_test_copy[\"question\"] + \" [SEP] \" + df_test_copy[\"context_trunc\"]\n",
    "\n",
    "# Tokenize\n",
    "test_encodings = w38_tokenizer(\n",
    "    df_test_copy[\"text\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_SEQ_LEN_TRANSFORMER,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "w38_predictions = []\n",
    "w38_true_labels = df_test_copy[\"answerable\"].astype(int).tolist()\n",
    "\n",
    "print(f\"Making predictions on {len(df_test)} test examples...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(test_encodings[\"input_ids\"]), 16):\n",
    "        batch_input_ids = test_encodings[\"input_ids\"][i:i+16].to(device)\n",
    "        batch_attention_mask = test_encodings[\"attention_mask\"][i:i+16].to(device)\n",
    "\n",
    "        outputs = w38_model(\n",
    "            input_ids=batch_input_ids,\n",
    "            attention_mask=batch_attention_mask\n",
    "        )\n",
    "\n",
    "        logits = outputs.logits\n",
    "        batch_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        w38_predictions.extend(batch_preds.tolist())\n",
    "\n",
    "# Calculate metrics\n",
    "w38_accuracy = accuracy_score(w38_true_labels, w38_predictions)\n",
    "w38_precision = precision_score(w38_true_labels, w38_predictions, zero_division=0)\n",
    "w38_recall = recall_score(w38_true_labels, w38_predictions, zero_division=0)\n",
    "w38_f1 = f1_score(w38_true_labels, w38_predictions, zero_division=0)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nResults on {len(df_test)} test questions:\")\n",
    "print(f\"  Accuracy:  {w38_accuracy:.4f}\")\n",
    "print(f\"  Precision: {w38_precision:.4f}\")\n",
    "print(f\"  Recall:    {w38_recall:.4f}\")\n",
    "print(f\"  F1-score:  {w38_f1:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(w38_true_labels, w38_predictions)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TN: {cm[0,0]:3d}  FP: {cm[0,1]:3d}\")\n",
    "print(f\"  FN: {cm[1,0]:3d}  TP: {cm[1,1]:3d}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "for i in range(min(5, len(df_test))):\n",
    "    row = df_test.iloc[i]\n",
    "    pred = w38_predictions[i]\n",
    "    true = w38_true_labels[i]\n",
    "    correct = \"âœ“\" if pred == true else \"âœ—\"\n",
    "\n",
    "    print(f\"\\nExample {i+1} {correct}:\")\n",
    "    print(f\"  Question: {row['question'][:80]}...\")\n",
    "    print(f\"  Language: {row['lang']}\")\n",
    "    print(f\"  Predicted: {'Answerable' if pred else 'Not answerable'}\")\n",
    "    print(f\"  True:      {'Answerable' if true else 'Not answerable'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4176f98c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Week 39: mT5 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c256a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at mrm8488/mT5-small-finetuned-tydiqa-for-xqa were not used when initializing T5ForConditionalGeneration: ['decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at mrm8488/mT5-small-finetuned-tydiqa-for-xqa and are newly initialized: ['decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.0.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.1.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Week-39 base mT5 (Model 1 style) on EN/EL test set...\n",
      "\n",
      "W39 base mT5 on custom test â€” EM: 0.4000  |  F1: 0.5086\n",
      "\n",
      "Example 1 [en]: Who wrote the novel '1984'?...\n",
      "Pred: George Orwell. The The the the The The The The The The The The The The The The the the the The The The The the fiction the the the the The The The The The 1984 is\n",
      "Gold: George Orwell\n",
      "\n",
      "Example 2 [en]: What is the capital city of Japan?...\n",
      "Pred: Tokyo\n",
      "Gold: Tokyo\n",
      "\n",
      "Example 3 [en]: When did World War II end?...\n",
      "Pred: the the the the the the the the of the Axis powers\n",
      "Gold: 1945\n",
      "\n",
      "Example 4 [en]: Which planet is known as the Red Planet?...\n",
      "Pred: the Sun and is the fourth planet from the the Sun and is the fourth planet from the the Sun and is the fourth planet from the the the the the Sun and is the four the planet from the the the the the the the the the the Sun and is the the four the planet from the the the\n",
      "Gold: Mars\n",
      "\n",
      "Example 5 [en]: Who painted the Mona Lisa?...\n",
      "Pred: the the the the The The The Thomas\n",
      "Gold: Leonardo da Vinci\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load base mT5 TyDiQA model\n",
    "MT5 = \"mrm8488/mT5-small-finetuned-tydiqa-for-xqa\"\n",
    "tok = AutoTokenizer.from_pretrained(MT5)\n",
    "mt5 = AutoModelForSeq2SeqLM.from_pretrained(MT5).to(device).eval()\n",
    "\n",
    "\n",
    "# Simple normalization + metrics (EM, token-F1)\n",
    "def normalize(s):\n",
    "    if s is None: return \"\"\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n",
    "    s = s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def exact_match(p, g): return 1.0 if normalize(p) == normalize(g) else 0.0\n",
    "\n",
    "def token_f1(p, g):\n",
    "    p, g = normalize(p).split(), normalize(g).split()\n",
    "    if not p and not g: return 1.0\n",
    "    if not p or not g:  return 0.0\n",
    "    overlap = len(set(p) & set(g))\n",
    "    if overlap == 0: return 0.0\n",
    "    prec, rec = overlap/len(set(p)), overlap/len(set(g))\n",
    "    return 2*prec*rec/(prec+rec)\n",
    "\n",
    "# Evaluate\n",
    "preds, golds, ems, f1s = [], [], [], []\n",
    "print(\"\\nEvaluating Week-39 base mT5 (Model 1 style) on EN/EL test set...\")\n",
    "for _, row in df_test.iterrows():\n",
    "    q, ctx = str(row[\"question\"]), str(row[\"context\"])\n",
    "    is_ans = bool(row.get(\"answerable\", False))\n",
    "    gold = str(row.get(\"answer\", \"\")) if is_ans else \"\"\n",
    "\n",
    "    \n",
    "    prompt = f\"question: {q} context: {ctx[:400]}\"\n",
    "    inputs = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = mt5.generate(**inputs, max_new_tokens=64, num_beams=4, early_stopping=True)\n",
    "    pred = tok.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    preds.append(pred)\n",
    "    golds.append(gold)\n",
    "    ems.append(exact_match(pred if is_ans else \"\", gold))\n",
    "    f1s.append(token_f1(pred if is_ans else \"\", gold))\n",
    "\n",
    "w39_em = float(np.mean(ems))\n",
    "w39_f1 = float(np.mean(f1s))\n",
    "print(f\"\\nW39 base mT5 on custom test â€” EM: {w39_em:.4f}  |  F1: {w39_f1:.4f}\")\n",
    "\n",
    "for i in range(min(5, len(df_test))):\n",
    "    row = df_test.iloc[i]\n",
    "    print(f\"\\nExample {i+1} [{row.get('lang')}]: {row['question'][:100]}...\")\n",
    "    print(\"Pred:\", preds[i])\n",
    "    print(\"Gold:\", golds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e331d5",
   "metadata": {
    "id": "67e331d5"
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Week 40: Sequence Labeling for Answer Extraction\n",
    "\n",
    "Using BIO tagging with XLM-RoBERTa for extractive question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1e4411",
   "metadata": {
    "id": "8f1e4411"
   },
   "source": [
    "### 4.1 Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3407c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1761434383230,
     "user": {
      "displayName": "Themis Ch",
      "userId": "10107086598836666206"
     },
     "user_tz": -120
    },
    "id": "a5d3407c",
    "outputId": "79cc00a6-874c-4802-eff1-b80645964b70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def char_to_token_labels(context, answer_start, answer_text, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert character-level answer indices to token-level BIO labels.\n",
    "    \"\"\"\n",
    "    # Tokenize context\n",
    "    encoding = tokenizer(context, add_special_tokens=False, return_offsets_mapping=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "\n",
    "    # Initialize all labels as \"O\" (outside)\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    # If answerable, find answer span\n",
    "    if answer_start >= 0 and answer_text:\n",
    "        answer_end = answer_start + len(answer_text)\n",
    "        found_start = False\n",
    "\n",
    "        for i, (start, end) in enumerate(offsets):\n",
    "            # Check if token overlaps with answer\n",
    "            if start < answer_end and end > answer_start:\n",
    "                if not found_start:\n",
    "                    labels[i] = \"B-ANS\"  # First token of answer\n",
    "                    found_start = True\n",
    "                else:\n",
    "                    labels[i] = \"I-ANS\"  # Inside answer\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "\n",
    "def extract_answer_from_bio(tokens, labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Extract answer text from BIO labels.\n",
    "    \"\"\"\n",
    "    answer_tokens = []\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if label.startswith(\"B-\") or label.startswith(\"I-\"):\n",
    "            answer_tokens.append(token)\n",
    "\n",
    "    if not answer_tokens:\n",
    "        return \"\"\n",
    "\n",
    "    # Decode tokens to text\n",
    "    answer_text = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "    return answer_text.strip()\n",
    "\n",
    "\n",
    "def calculate_f1_token_overlap(pred_text, gold_text):\n",
    "    \"\"\"\n",
    "    Calculate F1 score based on token overlap.\n",
    "    \"\"\"\n",
    "    pred_tokens = set(pred_text.lower().split())\n",
    "    gold_tokens = set(gold_text.lower().split())\n",
    "\n",
    "    if len(pred_tokens) == 0 and len(gold_tokens) == 0:\n",
    "        return 1.0\n",
    "    elif len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    overlap = len(pred_tokens & gold_tokens)\n",
    "    precision = overlap / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
    "    recall = overlap / len(gold_tokens) if len(gold_tokens) > 0 else 0\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0656b",
   "metadata": {
    "id": "02f0656b"
   },
   "source": [
    "### 4.2 Load Pre-trained Sequence Labeling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383d227",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3850,
     "status": "ok",
     "timestamp": 1761434387081,
     "user": {
      "displayName": "Themis Ch",
      "userId": "10107086598836666206"
     },
     "user_tz": -120
    },
    "id": "c383d227",
    "outputId": "90a6e97c-c77c-4dd7-c3b1-32ca2d98211e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained BIO tagger from /content/drive/MyDrive/NLP-Project/models/xlmr_bio_tagger...\n",
      "âœ“ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# BIO label mappings\n",
    "LABEL_MAP = {\"O\": 0, \"B-ANS\": 1, \"I-ANS\": 2}\n",
    "ID2LABEL = {v: k for k, v in LABEL_MAP.items()}\n",
    "\n",
    "# Model path\n",
    "if IN_COLAB:\n",
    "    W40_MODEL_PATH = \"/content/drive/MyDrive/NLP-Project/models/xlmr_bio_tagger\"\n",
    "else:\n",
    "    W40_MODEL_PATH = \"./models/xlmr_bio_tagger\"\n",
    "\n",
    "# Try to load saved model\n",
    "if os.path.exists(W40_MODEL_PATH):\n",
    "    print(f\"Loading pre-trained BIO tagger from {W40_MODEL_PATH}...\")\n",
    "    w40_tokenizer = AutoTokenizer.from_pretrained(W40_MODEL_PATH)\n",
    "    w40_model = AutoModelForTokenClassification.from_pretrained(W40_MODEL_PATH)\n",
    "    w40_model.to(device)\n",
    "    w40_model.eval()\n",
    "    print(\"Model loaded successfully\")\n",
    "else:\n",
    "    print(f\"Model not found at {W40_MODEL_PATH}\")\n",
    "    print(\"  Using base XLM-R model (not fine-tuned)\")\n",
    "    w40_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    w40_model = AutoModelForTokenClassification.from_pretrained(\n",
    "        \"xlm-roberta-base\",\n",
    "        num_labels=len(LABEL_MAP),\n",
    "        id2label=ID2LABEL,\n",
    "        label2id=LABEL_MAP\n",
    "    )\n",
    "    w40_model.to(device)\n",
    "    w40_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f184d50",
   "metadata": {
    "id": "6f184d50"
   },
   "source": [
    "### 4.3 Evaluate W40 Sequence Labeler on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84346366",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17824,
     "status": "ok",
     "timestamp": 1761434405018,
     "user": {
      "displayName": "Themis Ch",
      "userId": "10107086598836666206"
     },
     "user_tz": -120
    },
    "id": "84346366",
    "outputId": "148b05d9-7c0b-4352-f3b5-926e6aeb1d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "WEEK 40: SEQUENCE LABELING FOR ANSWER EXTRACTION\n",
      "======================================================================\n",
      "Preparing test data with answer positions...\n",
      "\n",
      "Making predictions on 30 test examples...\n",
      "\n",
      "ðŸ“Š Results on 30 test questions:\n",
      "  Exact Match (EM): 0.2667\n",
      "  F1-score:         0.2667\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE PREDICTIONS\n",
      "======================================================================\n",
      "\n",
      "Example 1 âœ—:\n",
      "  Question: Who wrote the novel '1984'?...\n",
      "  Language: en\n",
      "  Predicted: ''\n",
      "  Gold:      'George Orwell'\n",
      "  EM: 0.00, F1: 0.0000\n",
      "\n",
      "Example 2 âœ“:\n",
      "  Question: What is the capital city of Japan?...\n",
      "  Language: en\n",
      "  Predicted: 'Tokyo'\n",
      "  Gold:      'Tokyo'\n",
      "  EM: 1.00, F1: 1.0000\n",
      "\n",
      "Example 3 âœ—:\n",
      "  Question: When did World War II end?...\n",
      "  Language: en\n",
      "  Predicted: ''\n",
      "  Gold:      '1945'\n",
      "  EM: 0.00, F1: 0.0000\n",
      "\n",
      "Example 4 âœ—:\n",
      "  Question: Which planet is known as the Red Planet?...\n",
      "  Language: en\n",
      "  Predicted: ''\n",
      "  Gold:      'Mars'\n",
      "  EM: 0.00, F1: 0.0000\n",
      "\n",
      "Example 5 âœ—:\n",
      "  Question: Who painted the Mona Lisa?...\n",
      "  Language: en\n",
      "  Predicted: ''\n",
      "  Gold:      'Leonardo da Vinci'\n",
      "  EM: 0.00, F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"WEEK 40: SEQUENCE LABELING FOR ANSWER EXTRACTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare data: need to add answer_start field\n",
    "print(\"Preparing test data with answer positions...\")\n",
    "\n",
    "# For questions.json, we need to find answer_start\n",
    "for idx, row in df_test.iterrows():\n",
    "    if row['answerable'] and row['answer']:\n",
    "        # Find the answer in the context\n",
    "        answer_pos = row['context'].find(row['answer'])\n",
    "        if answer_pos == -1:\n",
    "            # Try case-insensitive search\n",
    "            answer_pos = row['context'].lower().find(row['answer'].lower())\n",
    "        df_test.loc[idx, 'answer_start'] = answer_pos if answer_pos >= 0 else 0\n",
    "    else:\n",
    "        df_test.loc[idx, 'answer_start'] = -1\n",
    "\n",
    "# Convert to int\n",
    "df_test['answer_start'] = df_test['answer_start'].fillna(-1).astype(int)\n",
    "\n",
    "# Make predictions\n",
    "w40_pred_answers = []\n",
    "w40_gold_answers = []\n",
    "w40_f1_scores = []\n",
    "w40_exact_matches = []\n",
    "\n",
    "print(f\"\\nMaking predictions on {len(df_test)} test examples...\")\n",
    "\n",
    "for idx, row in df_test.iterrows():\n",
    "    # Tokenize context\n",
    "    tokens, gold_labels = char_to_token_labels(\n",
    "        row['context'],\n",
    "        row['answer_start'],\n",
    "        row['answer'] if row['answerable'] else \"\",\n",
    "        w40_tokenizer\n",
    "    )\n",
    "\n",
    "    # Encode for model\n",
    "    encoding = w40_tokenizer(\n",
    "        row['context'],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = w40_model(\n",
    "            input_ids=encoding['input_ids'].to(device),\n",
    "            attention_mask=encoding['attention_mask'].to(device)\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "    # Convert predictions to labels\n",
    "    pred_label_ids = predictions[0].cpu().numpy()[:len(tokens)]\n",
    "    pred_labels = [ID2LABEL[id] for id in pred_label_ids]\n",
    "\n",
    "    # Extract answer\n",
    "    pred_answer = extract_answer_from_bio(tokens, pred_labels, w40_tokenizer)\n",
    "\n",
    "    if row['answerable']:\n",
    "        ctx_lower = row['context'].lower()\n",
    "        gold_candidates = [row.get('answer', ''), row.get('answer_inlang', '')]\n",
    "        gold_answer = \"\"\n",
    "        for cand in gold_candidates:\n",
    "            if cand and cand.lower() in ctx_lower:\n",
    "                gold_answer = cand\n",
    "                break\n",
    "        if not gold_answer:\n",
    "            gold_answer = row.get('answer', '')\n",
    "    else:\n",
    "        gold_answer = \"\"\n",
    "\n",
    "    w40_pred_answers.append(pred_answer)\n",
    "    w40_gold_answers.append(gold_answer)\n",
    "\n",
    "    # Calculate metrics\n",
    "    exact_match = 1.0 if pred_answer.strip() == gold_answer.strip() else 0.0\n",
    "    f1 = calculate_f1_token_overlap(pred_answer, gold_answer)\n",
    "\n",
    "    w40_exact_matches.append(exact_match)\n",
    "    w40_f1_scores.append(f1)\n",
    "\n",
    "# Calculate overall metrics\n",
    "w40_em = np.mean(w40_exact_matches)\n",
    "w40_f1_avg = np.mean(w40_f1_scores)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nResults on {len(df_test)} test questions:\")\n",
    "print(f\"  Exact Match (EM): {w40_em:.4f}\")\n",
    "print(f\"  F1-score:         {w40_f1_avg:.4f}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "for i in range(min(5, len(df_test))):\n",
    "    row = df_test.iloc[i]\n",
    "    pred = w40_pred_answers[i]\n",
    "    gold = w40_gold_answers[i]\n",
    "    em = w40_exact_matches[i]\n",
    "    f1 = w40_f1_scores[i]\n",
    "    correct = \"âœ“\" if em == 1.0 else \"âœ—\"\n",
    "\n",
    "    print(f\"\\nExample {i+1} {correct}:\")\n",
    "    print(f\"  Question: {row['question'][:80]}...\")\n",
    "    print(f\"  Language: {row['lang']}\")\n",
    "    print(f\"  Predicted: '{pred}'\")\n",
    "    print(f\"  Gold:      '{gold}'\")\n",
    "    print(f\"  EM: {em:.2f}, F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d71d9",
   "metadata": {
    "id": "2c8d71d9"
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Summary and Comparison\n",
    "\n",
    "Comparing all models on the custom test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d4928",
   "metadata": {
    "id": "fc8d4928"
   },
   "source": [
    "### 5.1 Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "94faed27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1761434405027,
     "user": {
      "displayName": "Themis Ch",
      "userId": "10107086598836666206"
     },
     "user_tz": -120
    },
    "id": "94faed27",
    "outputId": "df8b11d6-58cf-420d-b069-128f25a94210"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Week                    Model              Task Accuracy Precision Recall     F1     EM\n",
      " W36        Rule-based TF-IDF     Answerability   0.3000    1.0000 0.0455 0.0870      -\n",
      " W38 XLM-RoBERTa (fine-tuned)     Answerability   0.4667    1.0000 0.2727 0.4286      -\n",
      " W40         XLM-R BIO Tagger Answer Extraction        -         -      - 0.2667 0.2667\n",
      "\n",
      "======================================================================\n",
      "KEY OBSERVATIONS\n",
      "======================================================================\n",
      "\n",
      "Total test questions: 30\n",
      "\n",
      "Answerability Classification (W36 vs W38):\n",
      "  W36 Rule-based:    F1=0.0870, Accuracy=0.3000\n",
      "  W38 XLM-RoBERTa:   F1=0.4286, Accuracy=0.4667\n",
      "  â†’ XLM-RoBERTa performs better by 0.3416 F1 points\n",
      "\n",
      "Answer Extraction (W40):\n",
      "  XLM-R BIO Tagger:  F1=0.2667, EM=0.2667\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create results summary\n",
    "results_summary = []\n",
    "\n",
    "# Week 36 results\n",
    "results_summary.append({\n",
    "    'Week': 'W36',\n",
    "    'Model': 'Rule-based TF-IDF',\n",
    "    'Task': 'Answerability',\n",
    "    'Accuracy': f'{w36_accuracy:.4f}',\n",
    "    'Precision': f'{w36_precision:.4f}',\n",
    "    'Recall': f'{w36_recall:.4f}',\n",
    "    'F1': f'{w36_f1:.4f}',\n",
    "    'EM': '-',\n",
    "})\n",
    "\n",
    "# Week 38 results\n",
    "results_summary.append({\n",
    "    'Week': 'W38',\n",
    "    'Model': 'XLM-RoBERTa (fine-tuned)',\n",
    "    'Task': 'Answerability',\n",
    "    'Accuracy': f'{w38_accuracy:.4f}',\n",
    "    'Precision': f'{w38_precision:.4f}',\n",
    "    'Recall': f'{w38_recall:.4f}',\n",
    "    'F1': f'{w38_f1:.4f}',\n",
    "    'EM': '-',\n",
    "})\n",
    "\n",
    "# Week 40 results\n",
    "results_summary.append({\n",
    "    'Week': 'W40',\n",
    "    'Model': 'XLM-R BIO Tagger',\n",
    "    'Task': 'Answer Extraction',\n",
    "    'Accuracy': '-',\n",
    "    'Precision': '-',\n",
    "    'Recall': '-',\n",
    "    'F1': f'{w40_f1_avg:.4f}',\n",
    "    'EM': f'{w40_em:.4f}',\n",
    "})\n",
    "\n",
    "# Display as DataFrame\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "print(\"\\n\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visual comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY OBSERVATIONS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal test questions: {len(df_test)}\")\n",
    "print(f\"\\nAnswerability Classification (W36 vs W38):\")\n",
    "print(f\"  W36 Rule-based:    F1={w36_f1:.4f}, Accuracy={w36_accuracy:.4f}\")\n",
    "print(f\"  W38 XLM-RoBERTa:   F1={w38_f1:.4f}, Accuracy={w38_accuracy:.4f}\")\n",
    "if w38_f1 > w36_f1:\n",
    "    print(f\"  â†’ XLM-RoBERTa performs better by {(w38_f1-w36_f1):.4f} F1 points\")\n",
    "else:\n",
    "    print(f\"  â†’ Rule-based performs better by {(w36_f1-w38_f1):.4f} F1 points\")\n",
    "\n",
    "print(f\"\\nAnswer Extraction (W40):\")\n",
    "print(f\"  XLM-R BIO Tagger:  F1={w40_f1_avg:.4f}, EM={w40_em:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
