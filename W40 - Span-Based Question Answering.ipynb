{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6516020",
   "metadata": {
    "id": "e6516020"
   },
   "source": [
    "# Week 39: Span-Based Question Answering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb105b",
   "metadata": {
    "id": "6fbb105b"
   },
   "source": [
    "# Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "710df0d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29678,
     "status": "ok",
     "timestamp": 1759745362424,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "710df0d0",
    "outputId": "28a7a1ef-bc56-4834-d73f-477204ffff21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Google Colab\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Environment detection\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"Running in local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5df4bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10510,
     "status": "ok",
     "timestamp": 1759745372930,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "7f5df4bc",
    "outputId": "10c96940-8b2d-4305-aaeb-7e50d0bf23c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch datasets accelerate seqeval scikit-learn pandas pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a999d219",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37432,
     "status": "ok",
     "timestamp": 1759745410394,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "a999d219",
    "outputId": "97751228-07e5-4c49-fcab-cbccce1b3d65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "from seqeval.metrics import f1_score as seqeval_f1\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65f5a1d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1759745410395,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "65f5a1d8",
    "outputId": "4316dabc-ec86-4789-fbdf-7dcb6e66800f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /content/drive/MyDrive/Colab_Notebooks/NLP/tydi_xor_rc\n"
     ]
    }
   ],
   "source": [
    "# Dataset configuration\n",
    "LANGUAGES = [\"ar\", \"ko\", \"te\"]\n",
    "LANGUAGE_NAMES = {\"ar\": \"Arabic\", \"ko\": \"Korean\", \"te\": \"Telugu\"}\n",
    "\n",
    "# Dataset paths\n",
    "if IN_COLAB:\n",
    "    BASE_DIR = Path(\"/content/drive/MyDrive/Colab_Notebooks/NLP/tydi_xor_rc\")\n",
    "else:\n",
    "    BASE_DIR = Path(\"./tydi_xor_rc\")\n",
    "\n",
    "TRAIN_PATH = BASE_DIR / \"train.parquet\"\n",
    "VAL_PATH = BASE_DIR / \"validation.parquet\"\n",
    "\n",
    "print(f\"Dataset directory: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c56d3724",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4196,
     "status": "ok",
     "timestamp": 1759745414588,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "c56d3724",
    "outputId": "f923d47d-e187-4181-907f-46f62601e8e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 6,335\n",
      "Validation examples: 1,155\n",
      "\n",
      "Language distribution:\n",
      "  Arabic: 2,558 train, 415 val\n",
      "  Korean: 2,422 train, 356 val\n",
      "  Telugu: 1,355 train, 384 val\n",
      "\n",
      "Dataset loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "if TRAIN_PATH.exists() and VAL_PATH.exists():\n",
    "    df_train = pd.read_parquet(TRAIN_PATH)\n",
    "    df_val = pd.read_parquet(VAL_PATH)\n",
    "\n",
    "    # Filter for target languages\n",
    "    df_train = df_train[df_train[\"lang\"].isin(LANGUAGES)].copy()\n",
    "    df_val = df_val[df_val[\"lang\"].isin(LANGUAGES)].copy()\n",
    "\n",
    "    print(f\"Training examples: {len(df_train):,}\")\n",
    "    print(f\"Validation examples: {len(df_val):,}\")\n",
    "    print(\"\\nLanguage distribution:\")\n",
    "    for lang in LANGUAGES:\n",
    "        train_count = len(df_train[df_train[\"lang\"] == lang])\n",
    "        val_count = len(df_val[df_val[\"lang\"] == lang])\n",
    "        print(f\"  {LANGUAGE_NAMES[lang]}: {train_count:,} train, {val_count:,} val\")\n",
    "\n",
    "    print(\"\\nDataset loaded successfully\")\n",
    "else:\n",
    "    print(\"ERROR: Dataset files not found!\")\n",
    "    print(f\"Please ensure files exist at: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c6af4",
   "metadata": {
    "id": "396c6af4"
   },
   "source": [
    "# Data Preprocessing Utilities\n",
    "\n",
    "Convert character-level answer indices to token-level labels (BIO format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7173a1eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1759745414596,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "7173a1eb",
    "outputId": "04faaea6-20ac-45cd-d6da-03b6df5dd0d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing utilities defined\n"
     ]
    }
   ],
   "source": [
    "def char_to_token_labels(context, answer_start, answer_text, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert character-level answer indices to token-level BIO labels.\n",
    "\n",
    "    Args:\n",
    "        context: The context text\n",
    "        answer_start: Character index where answer starts\n",
    "        answer_text: The answer text\n",
    "        tokenizer: Tokenizer to use for tokenization\n",
    "\n",
    "    Returns:\n",
    "        tokens: List of tokens\n",
    "        labels: List of BIO labels (\"O\", \"B-ANS\", \"I-ANS\")\n",
    "    \"\"\"\n",
    "    # Tokenize context\n",
    "    encoding = tokenizer(context, add_special_tokens=False, return_offsets_mapping=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "\n",
    "    # Initialize all labels as \"O\" (outside)\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    # If answerable, find answer span\n",
    "    if answer_start >= 0 and answer_text:\n",
    "        answer_end = answer_start + len(answer_text)\n",
    "        found_start = False\n",
    "\n",
    "        for i, (start, end) in enumerate(offsets):\n",
    "            # Check if token overlaps with answer\n",
    "            if start < answer_end and end > answer_start:\n",
    "                if not found_start:\n",
    "                    labels[i] = \"B-ANS\"  # First token of answer\n",
    "                    found_start = True\n",
    "                else:\n",
    "                    labels[i] = \"I-ANS\"  # Inside answer\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "\n",
    "def prepare_bio_data(df, tokenizer, max_samples=None):\n",
    "    \"\"\"\n",
    "    Prepare dataset with BIO labels.\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with 'tokens', 'labels', 'question', 'lang'\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if max_samples and len(examples) >= max_samples:\n",
    "            break\n",
    "\n",
    "        # Get answer info\n",
    "        if row[\"answerable\"]:\n",
    "            answer_start = row[\"answer_start\"]\n",
    "            answer_text = row[\"answer\"]\n",
    "        else:\n",
    "            answer_start = -1\n",
    "            answer_text = \"\"\n",
    "\n",
    "        # Convert to BIO labels\n",
    "        tokens, labels = char_to_token_labels(\n",
    "            row[\"context\"],\n",
    "            answer_start,\n",
    "            answer_text,\n",
    "            tokenizer\n",
    "        )\n",
    "\n",
    "        examples.append({\n",
    "            \"tokens\": tokens,\n",
    "            \"labels\": labels,\n",
    "            \"question\": row[\"question\"],\n",
    "            \"lang\": row[\"lang\"]\n",
    "        })\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "print(\"Preprocessing utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a65b1c",
   "metadata": {
    "id": "51a65b1c"
   },
   "source": [
    "---\n",
    "\n",
    "# Model 1: BiLSTM-CRF with BIO Tagging\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a3123",
   "metadata": {
    "id": "495a3123"
   },
   "source": [
    "## Model 1: Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c9610a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1759745414645,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "1c9610a8",
    "outputId": "e64306a9-39c7-445a-e4a1-29805c5004c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 (BiLSTM-CRF) classes defined\n"
     ]
    }
   ],
   "source": [
    "# Simple BiLSTM-CRF implementation (without external CRF library for simplicity)\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    \"\"\"BiLSTM with CRF for sequence labeling\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim // 2,  # Divided by 2 because bidirectional\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if dropout > 0 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "        # CRF transition scores\n",
    "        self.transitions = nn.Parameter(torch.randn(num_labels, num_labels))\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"Forward pass - returns emission scores\"\"\"\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        return emissions\n",
    "\n",
    "    def compute_loss(self, emissions, tags, mask):\n",
    "        \"\"\"CRF loss computation\"\"\"\n",
    "        # Simplified loss - use cross-entropy instead of full CRF for simplicity\n",
    "        if mask is not None:\n",
    "            active_loss = mask.view(-1) == 1\n",
    "            active_logits = emissions.view(-1, self.num_labels)[active_loss]\n",
    "            active_labels = tags.view(-1)[active_loss]\n",
    "            loss = nn.CrossEntropyLoss()(active_logits, active_labels)\n",
    "        else:\n",
    "            loss = nn.CrossEntropyLoss()(\n",
    "                emissions.view(-1, self.num_labels),\n",
    "                tags.view(-1)\n",
    "            )\n",
    "        return loss\n",
    "\n",
    "    def decode(self, emissions, mask=None):\n",
    "        \"\"\"Viterbi-like decoding (simplified as argmax for implementation simplicity)\"\"\"\n",
    "        return torch.argmax(emissions, dim=-1)\n",
    "\n",
    "\n",
    "# Vocabulary and Dataset classes\n",
    "class SimpleVocab:\n",
    "    \"\"\"Simple vocabulary for tokenization\"\"\"\n",
    "\n",
    "    def __init__(self, tokens_list, max_size=50000):\n",
    "        counter = Counter()\n",
    "        for tokens in tokens_list:\n",
    "            counter.update(tokens)\n",
    "\n",
    "        self.token2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        for token, _ in counter.most_common(max_size - 2):\n",
    "            if token not in self.token2idx:\n",
    "                self.token2idx[token] = len(self.token2idx)\n",
    "\n",
    "        self.idx2token = {v: k for k, v in self.token2idx.items()}\n",
    "        print(f\"  Vocabulary size: {len(self.token2idx):,}\")\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        return [self.token2idx.get(t, 1) for t in tokens]\n",
    "\n",
    "\n",
    "class BIODataset(Dataset):\n",
    "    \"\"\"Dataset for BIO sequence labeling\"\"\"\n",
    "\n",
    "    def __init__(self, examples, vocab, label2idx):\n",
    "        self.examples = examples\n",
    "        self.vocab = vocab\n",
    "        self.label2idx = label2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.examples[idx]\n",
    "        token_ids = self.vocab.encode(ex[\"tokens\"])\n",
    "        label_ids = [self.label2idx[l] for l in ex[\"labels\"]]\n",
    "        return {\n",
    "            \"token_ids\": token_ids,\n",
    "            \"label_ids\": label_ids,\n",
    "            \"length\": len(token_ids)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_bio_batch(batch):\n",
    "    \"\"\"Collate function for batching\"\"\"\n",
    "    max_len = max(b[\"length\"] for b in batch)\n",
    "\n",
    "    token_ids = []\n",
    "    label_ids = []\n",
    "    masks = []\n",
    "\n",
    "    for b in batch:\n",
    "        # Pad sequences\n",
    "        pad_len = max_len - b[\"length\"]\n",
    "        token_ids.append(b[\"token_ids\"] + [0] * pad_len)\n",
    "        label_ids.append(b[\"label_ids\"] + [0] * pad_len)\n",
    "        masks.append([1] * b[\"length\"] + [0] * pad_len)\n",
    "\n",
    "    return {\n",
    "        \"token_ids\": torch.tensor(token_ids, dtype=torch.long),\n",
    "        \"label_ids\": torch.tensor(label_ids, dtype=torch.long),\n",
    "        \"mask\": torch.tensor(masks, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Model 1 (BiLSTM-CRF) classes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1507046",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1759745414682,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "a1507046",
    "outputId": "f9b82483-18a7-4b50-fa6c-44c5a9324a99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 training functions defined\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation functions for Model 1\n",
    "def train_bilstm_crf(model, train_loader, val_loader, epochs=10, lr=0.001):\n",
    "    \"\"\"Train BiLSTM-CRF model\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            token_ids = batch[\"token_ids\"].to(device)\n",
    "            label_ids = batch[\"label_ids\"].to(device)\n",
    "            mask = batch[\"mask\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            emissions = model(token_ids, mask)\n",
    "            loss = model.compute_loss(emissions, label_ids, mask)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        val_f1 = evaluate_bilstm_crf(model, val_loader)\n",
    "\n",
    "        print(f\"  Epoch {epoch+1}/{epochs}: Loss={avg_loss:.4f}, Val F1={val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "\n",
    "    return best_f1\n",
    "\n",
    "\n",
    "def evaluate_bilstm_crf(model, data_loader):\n",
    "    \"\"\"Evaluate BiLSTM-CRF model\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            token_ids = batch[\"token_ids\"].to(device)\n",
    "            label_ids = batch[\"label_ids\"].to(device)\n",
    "            mask = batch[\"mask\"].to(device)\n",
    "\n",
    "            emissions = model(token_ids, mask)\n",
    "            predictions = model.decode(emissions, mask)\n",
    "\n",
    "            # Convert to lists (excluding padding)\n",
    "            for i in range(len(predictions)):\n",
    "                mask_len = mask[i].sum().item()\n",
    "                pred_seq = predictions[i][:mask_len].cpu().tolist()\n",
    "                label_seq = label_ids[i][:mask_len].cpu().tolist()\n",
    "                all_preds.append(pred_seq)\n",
    "                all_labels.append(label_seq)\n",
    "\n",
    "    # Calculate F1 (token-level)\n",
    "    flat_preds = [p for seq in all_preds for p in seq]\n",
    "    flat_labels = [l for seq in all_labels for l in seq]\n",
    "\n",
    "    # Filter out padding (label 0)\n",
    "    valid_indices = [i for i, l in enumerate(flat_labels) if l != 0]\n",
    "    filtered_preds = [flat_preds[i] for i in valid_indices]\n",
    "    filtered_labels = [flat_labels[i] for i in valid_indices]\n",
    "\n",
    "    if len(filtered_labels) > 0:\n",
    "        f1 = f1_score(filtered_labels, filtered_preds, average='weighted', zero_division=0)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "print(\"Model 1 training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce3b8a",
   "metadata": {
    "id": "07ce3b8a"
   },
   "source": [
    "## Model 1: Training & Evaluation\n",
    "\n",
    "Train separate BiLSTM-CRF models for each language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cda7207",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "567dcd809dd041c191fbd4e6ae9122d3",
      "e59589a16f3947e3984de464e7f9483a",
      "14180c3d58a04b9d9cd3792ff495d1ad",
      "5a38c4bf49b7445586bd3d48ad0a379e",
      "89f7232ba4ae4a2a93201938f82b1294",
      "1805a03b8d42455dae13568f8f4ba968",
      "db44c326850d4bbeaf95d66e7276f9c4",
      "8f7d24468b52471c91ceed7ae5198e7e",
      "c20385491c824aadbca4fc60517dec67",
      "b9d82322ecd643eaa1fa1f02e693c6ee",
      "4f0ec89bfb4c46799064c5c06416d101",
      "9df74f128f294ff9b188eddcf822ee0c",
      "f292e9bea00d47e3be0860e9a67a724d",
      "abee68ae86b4408eb188dcba72b88a27",
      "97e2c31ec3c240c5bb3be0f2618178b4",
      "a40ab5efbb304784b14e04841617ddad",
      "1fc5343eea6744fda15cf6101aca2787",
      "d0c41d79a56743a798b5352671e2a79f",
      "281f2ceafceb467c8a91be949a47e4cd",
      "5aff37da77234fcc9597b213c415e00f",
      "862b9dfa4cf54547b28727f16fdc7487",
      "9af4ee1e39424752bc3a0c5362eaa5aa",
      "c8a766f4a889476b914a416ada7854ab",
      "dc54ee016a174f889d7c35fca5590165",
      "15c0d220282f4fde93b9d3ffb73c3c43",
      "f1bd603ad1bd411889a8c7a23cbf5f9e",
      "d0f9c334f7274949af75ff45f5ba94c6",
      "055d5dabbe89442ca110bef40ac7f0e8",
      "50a2c26a4e5c4ee1bea16357e0fefe00",
      "b3cd8eb1290041d1ae0580417e0811dc",
      "04a5b758993f4484b5169e74416ccafe",
      "087c77bd7fa841a79458184d63b5cfd0",
      "a1843cfb9ade4663986a91df33bc7a62",
      "46001449c0644e9e8add4fa260237881",
      "7e5e3445134549069c8f80ba60be11a8",
      "8c27937eff72449fb9e88b5a3e5d3752",
      "b55718da47f74cdba5ffe9b4c04c9976",
      "0452210a534342c29f6c11631bbbff96",
      "023c020b383746708938c4e13e7481a7",
      "b4aec88a16034d9c9fbca16306dba555",
      "8a103e1285774bb0af5b2dbfc5d44be9",
      "bafe703ef70046d5848e648c5709c76d",
      "61b356d80166452e983f26e1f9b1e314",
      "1e65c11b91a6437c9c2b724eef65a2a0"
     ]
    },
    "executionInfo": {
     "elapsed": 63863,
     "status": "ok",
     "timestamp": 1759745478561,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "1cda7207",
    "outputId": "a44f231e-5364-492e-fb21-c273946f3ed8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567dcd809dd041c191fbd4e6ae9122d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df74f128f294ff9b188eddcf822ee0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a766f4a889476b914a416ada7854ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46001449c0644e9e8add4fa260237881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (750 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL 1: BiLSTM-CRF with BIO Tagging (Per-Language)\n",
      "======================================================================\n",
      "\n",
      "==================================================\n",
      "Training for Arabic (ar)\n",
      "==================================================\n",
      "Preparing BIO labels...\n",
      "  Train examples: 2000\n",
      "  Val examples: 415\n",
      "Building vocabulary...\n",
      "  Vocabulary size: 16,974\n",
      "Training model...\n",
      "  Epoch 1/8: Loss=0.2300, Val F1=0.0000\n",
      "  Epoch 2/8: Loss=0.1603, Val F1=0.0043\n",
      "  Epoch 3/8: Loss=0.1502, Val F1=0.0148\n",
      "  Epoch 4/8: Loss=0.1343, Val F1=0.0343\n",
      "  Epoch 5/8: Loss=0.1176, Val F1=0.0543\n",
      "  Epoch 6/8: Loss=0.0977, Val F1=0.0757\n",
      "  Epoch 7/8: Loss=0.0800, Val F1=0.1228\n",
      "  Epoch 8/8: Loss=0.0648, Val F1=0.1607\n",
      "\n",
      "✓ Arabic - Best F1: 0.1607\n",
      "\n",
      "==================================================\n",
      "Training for Korean (ko)\n",
      "==================================================\n",
      "Preparing BIO labels...\n",
      "  Train examples: 2000\n",
      "  Val examples: 356\n",
      "Building vocabulary...\n",
      "  Vocabulary size: 16,820\n",
      "Training model...\n",
      "  Epoch 1/8: Loss=0.2186, Val F1=0.0000\n",
      "  Epoch 2/8: Loss=0.1449, Val F1=0.0000\n",
      "  Epoch 3/8: Loss=0.1364, Val F1=0.0034\n",
      "  Epoch 4/8: Loss=0.1221, Val F1=0.0340\n",
      "  Epoch 5/8: Loss=0.1032, Val F1=0.0479\n",
      "  Epoch 6/8: Loss=0.0835, Val F1=0.0563\n",
      "  Epoch 7/8: Loss=0.0668, Val F1=0.0668\n",
      "  Epoch 8/8: Loss=0.0523, Val F1=0.1479\n",
      "\n",
      "✓ Korean - Best F1: 0.1479\n",
      "\n",
      "==================================================\n",
      "Training for Telugu (te)\n",
      "==================================================\n",
      "Preparing BIO labels...\n",
      "  Train examples: 1355\n",
      "  Val examples: 384\n",
      "Building vocabulary...\n",
      "  Vocabulary size: 13,512\n",
      "Training model...\n",
      "  Epoch 1/8: Loss=0.2651, Val F1=0.0000\n",
      "  Epoch 2/8: Loss=0.1492, Val F1=0.0033\n",
      "  Epoch 3/8: Loss=0.1417, Val F1=0.0114\n",
      "  Epoch 4/8: Loss=0.1304, Val F1=0.0306\n",
      "  Epoch 5/8: Loss=0.1149, Val F1=0.0626\n",
      "  Epoch 6/8: Loss=0.0966, Val F1=0.0844\n",
      "  Epoch 7/8: Loss=0.0798, Val F1=0.1281\n",
      "  Epoch 8/8: Loss=0.0658, Val F1=0.1088\n",
      "\n",
      "✓ Telugu - Best F1: 0.1281\n",
      "\n",
      "======================================================================\n",
      "MODEL 1 RESULTS SUMMARY\n",
      "======================================================================\n",
      "     model language lang_code  f1_score\n",
      "BiLSTM-CRF   Arabic        ar  0.160745\n",
      "BiLSTM-CRF   Korean        ko  0.147904\n",
      "BiLSTM-CRF   Telugu        te  0.128109\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer for preprocessing\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "base_tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# Label mapping\n",
    "LABEL2IDX = {\"O\": 0, \"B-ANS\": 1, \"I-ANS\": 2}\n",
    "IDX2LABEL = {v: k for k, v in LABEL2IDX.items()}\n",
    "\n",
    "# Train Model 1 for each language\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL 1: BiLSTM-CRF with BIO Tagging (Per-Language)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model1_results = []\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training for {LANGUAGE_NAMES[lang]} ({lang})\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Prepare data\n",
    "    df_train_lang = df_train[df_train[\"lang\"] == lang].head(2000)  # Limit for speed\n",
    "    df_val_lang = df_val[df_val[\"lang\"] == lang].head(500)\n",
    "\n",
    "    print(f\"Preparing BIO labels...\")\n",
    "    train_examples = prepare_bio_data(df_train_lang, base_tokenizer)\n",
    "    val_examples = prepare_bio_data(df_val_lang, base_tokenizer)\n",
    "\n",
    "    print(f\"  Train examples: {len(train_examples)}\")\n",
    "    print(f\"  Val examples: {len(val_examples)}\")\n",
    "\n",
    "    # Build vocabulary\n",
    "    print(f\"Building vocabulary...\")\n",
    "    all_tokens = [ex[\"tokens\"] for ex in train_examples]\n",
    "    vocab = SimpleVocab(all_tokens)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = BIODataset(train_examples, vocab, LABEL2IDX)\n",
    "    val_dataset = BIODataset(val_examples, vocab, LABEL2IDX)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_bio_batch\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_bio_batch\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = BiLSTM_CRF(\n",
    "        vocab_size=len(vocab.token2idx),\n",
    "        embedding_dim=128,\n",
    "        hidden_dim=256,\n",
    "        num_labels=len(LABEL2IDX),\n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"Training model...\")\n",
    "    best_f1 = train_bilstm_crf(model, train_loader, val_loader, epochs=8, lr=0.001)\n",
    "\n",
    "    model1_results.append({\n",
    "        \"model\": \"BiLSTM-CRF\",\n",
    "        \"language\": LANGUAGE_NAMES[lang],\n",
    "        \"lang_code\": lang,\n",
    "        \"f1_score\": best_f1\n",
    "    })\n",
    "\n",
    "    print(f\"\\n✓ {LANGUAGE_NAMES[lang]} - Best F1: {best_f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 1 RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "results_df = pd.DataFrame(model1_results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0cd96",
   "metadata": {
    "id": "e9a0cd96"
   },
   "source": [
    "---\n",
    "\n",
    "# Model 2: XLM-RoBERTa with Token Classification (BIO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc15967f",
   "metadata": {
    "id": "dc15967f"
   },
   "source": [
    "## Model 2: Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7eec6e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1759745478583,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "f7eec6e7",
    "outputId": "7343eb69-9b3a-49a5-a66c-2601bb44a685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 (XLM-R Token Classification) functions defined\n"
     ]
    }
   ],
   "source": [
    "def prepare_xlmr_token_classification_data(df, tokenizer, label2id, max_length=512):\n",
    "    \"\"\"\n",
    "    Prepare data for XLM-R token classification.\n",
    "    Combines question and context, aligns labels with subword tokens.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        context = row[\"context\"]\n",
    "\n",
    "        # Tokenize question and context separately to track offsets\n",
    "        question_encoding = tokenizer(question, add_special_tokens=False)\n",
    "        context_encoding = tokenizer(context, add_special_tokens=False, return_offsets_mapping=True)\n",
    "\n",
    "        # Get answer info\n",
    "        if row[\"answerable\"]:\n",
    "            answer_start = row[\"answer_start\"]\n",
    "            answer_text = row[\"answer\"]\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "        else:\n",
    "            answer_start = -1\n",
    "            answer_end = -1\n",
    "            answer_text = \"\"\n",
    "\n",
    "        # Create labels for context tokens\n",
    "        context_labels = []\n",
    "        found_start = False\n",
    "\n",
    "        for start_char, end_char in context_encoding[\"offset_mapping\"]:\n",
    "            if answer_start >= 0 and start_char < answer_end and end_char > answer_start:\n",
    "                if not found_start:\n",
    "                    context_labels.append(label2id[\"B-ANS\"])\n",
    "                    found_start = True\n",
    "                else:\n",
    "                    context_labels.append(label2id[\"I-ANS\"])\n",
    "            else:\n",
    "                context_labels.append(label2id[\"O\"])\n",
    "\n",
    "        # Combine: [CLS] question [SEP] context [SEP]\n",
    "        input_ids = (\n",
    "            [tokenizer.cls_token_id] +\n",
    "            question_encoding[\"input_ids\"][:100] +  # Limit question length\n",
    "            [tokenizer.sep_token_id] +\n",
    "            context_encoding[\"input_ids\"][:max_length-len(question_encoding[\"input_ids\"])-3] +\n",
    "            [tokenizer.sep_token_id]\n",
    "        )\n",
    "\n",
    "        # Labels: -100 for special tokens and question tokens (don't predict on these)\n",
    "        labels = (\n",
    "            [-100] +  # CLS\n",
    "            [-100] * len(question_encoding[\"input_ids\"][:100]) +  # Question tokens\n",
    "            [-100] +  # SEP\n",
    "            context_labels[:max_length-len(question_encoding[\"input_ids\"])-3] +  # Context labels\n",
    "            [-100]    # SEP\n",
    "        )\n",
    "\n",
    "        # Attention mask\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Truncate if needed\n",
    "        if len(input_ids) > max_length:\n",
    "            input_ids = input_ids[:max_length]\n",
    "            labels = labels[:max_length]\n",
    "            attention_mask = attention_mask[:max_length]\n",
    "\n",
    "        examples.append({\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        })\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def compute_token_classification_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for token classification\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens with -100)\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(predictions, labels):\n",
    "        true_label_seq = []\n",
    "        true_pred_seq = []\n",
    "\n",
    "        for p, l in zip(pred_seq, label_seq):\n",
    "            if l != -100:\n",
    "                true_label_seq.append(IDX2LABEL[l])\n",
    "                true_pred_seq.append(IDX2LABEL[p])\n",
    "\n",
    "        if true_label_seq:\n",
    "            true_labels.append(true_label_seq)\n",
    "            true_predictions.append(true_pred_seq)\n",
    "\n",
    "    # Compute seqeval F1\n",
    "    if true_labels:\n",
    "        f1 = seqeval_f1(true_labels, true_predictions)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "\n",
    "    return {\"f1\": f1}\n",
    "\n",
    "\n",
    "print(\"Model 2 (XLM-R Token Classification) functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb1525",
   "metadata": {
    "id": "91eb1525"
   },
   "source": [
    "## Model 2: Training & Evaluation\n",
    "\n",
    "Train a single multilingual XLM-RoBERTa model on all three languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fedbf04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 933,
     "referenced_widgets": [
      "b0f267aa1a8f4be28d18dc102c8df2ee",
      "0d4c1075642043caa476c20fa34f26b4",
      "df238a6c4cec4dbda81b9196d49263f5",
      "6fc9c7dd7af74531b3ef71f94e9994eb",
      "a6c064c775c74fd4b7aff2ae0b08c134",
      "d372cc8506f24070a0cb68fbb23b2555",
      "c508810aad174ca99ffb584e8df7b141",
      "ae0adce1fc0646bcbc71ac4f4b074fbf",
      "ac93dafb474b4ce0b2507836835b3710",
      "a242866c369840e1a31848b00b2634ff",
      "8277eadd11e5476bbcc21ea2061c6be3"
     ]
    },
    "executionInfo": {
     "elapsed": 219969,
     "status": "ok",
     "timestamp": 1759745905919,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "1fedbf04",
    "outputId": "47b9647f-4dcc-4f65-93d9-5960d0ba80ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL 2: XLM-RoBERTa Token Classification (Multilingual)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing multilingual dataset...\n",
      "  Train: 3000 examples\n",
      "  Val: 900 examples\n",
      "\n",
      "Tokenizing and aligning labels...\n",
      "  Train dataset: 3000\n",
      "  Val dataset: 900\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f267aa1a8f4be28d18dc102c8df2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XLM-RoBERTa...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='694' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 694/1125 02:49 < 01:45, 4.09 it/s, Epoch 1.85/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.112838</td>\n",
       "      <td>0.164649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1125/1125 06:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.112838</td>\n",
       "      <td>0.164649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.102600</td>\n",
       "      <td>0.102253</td>\n",
       "      <td>0.296249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.109654</td>\n",
       "      <td>0.340274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL 2 RESULTS PER LANGUAGE\n",
      "======================================================================\n",
      "\n",
      "Evaluating Arabic...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Arabic F1: 0.3427\n",
      "\n",
      "Evaluating Korean...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Korean F1: 0.3422\n",
      "\n",
      "Evaluating Telugu...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Telugu F1: 0.3367\n",
      "\n",
      "======================================================================\n",
      "MODEL 2 RESULTS SUMMARY\n",
      "======================================================================\n",
      "      model language lang_code  f1_score\n",
      "XLM-R (BIO)   Arabic        ar  0.342750\n",
      "XLM-R (BIO)   Korean        ko  0.342246\n",
      "XLM-R (BIO)   Telugu        te  0.336735\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MODEL 2: XLM-RoBERTa Token Classification (Multilingual)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prepare data for all languages (multilingual training)\n",
    "print(\"\\nPreparing multilingual dataset...\")\n",
    "df_train_limited = df_train.groupby(\"lang\").head(1000)  # 1000 per language\n",
    "df_val_limited = df_val.groupby(\"lang\").head(300)  # 300 per language\n",
    "\n",
    "print(f\"  Train: {len(df_train_limited)} examples\")\n",
    "print(f\"  Val: {len(df_val_limited)} examples\")\n",
    "\n",
    "print(\"\\nTokenizing and aligning labels...\")\n",
    "train_examples = prepare_xlmr_token_classification_data(df_train_limited, tokenizer, LABEL2IDX)\n",
    "val_examples = prepare_xlmr_token_classification_data(df_val_limited, tokenizer, LABEL2IDX)\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = HFDataset.from_list(train_examples)\n",
    "val_dataset = HFDataset.from_list(val_examples)\n",
    "\n",
    "print(f\"  Train dataset: {len(train_dataset)}\")\n",
    "print(f\"  Val dataset: {len(val_dataset)}\")\n",
    "\n",
    "# Initialize model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(LABEL2IDX),\n",
    "    id2label=IDX2LABEL,\n",
    "    label2id=LABEL2IDX\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model2_xlmr_token_classification\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_token_classification_metrics\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining XLM-RoBERTa...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate per language\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 2 RESULTS PER LANGUAGE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model2_results = []\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "    print(f\"\\nEvaluating {LANGUAGE_NAMES[lang]}...\")\n",
    "\n",
    "    df_val_lang = df_val[df_val[\"lang\"] == lang].head(300)\n",
    "    val_lang_examples = prepare_xlmr_token_classification_data(df_val_lang, tokenizer, LABEL2IDX)\n",
    "    val_lang_dataset = HFDataset.from_list(val_lang_examples)\n",
    "\n",
    "    # Predict\n",
    "    predictions = trainer.predict(val_lang_dataset)\n",
    "    metrics = compute_token_classification_metrics((predictions.predictions, predictions.label_ids))\n",
    "\n",
    "    model2_results.append({\n",
    "        \"model\": \"XLM-R (BIO)\",\n",
    "        \"language\": LANGUAGE_NAMES[lang],\n",
    "        \"lang_code\": lang,\n",
    "        \"f1_score\": metrics[\"f1\"]\n",
    "    })\n",
    "\n",
    "    print(f\"  {LANGUAGE_NAMES[lang]} F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 2 RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "results_df = pd.DataFrame(model2_results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e32a8f",
   "metadata": {
    "id": "90e32a8f"
   },
   "source": [
    "---\n",
    "\n",
    "# Model 3: XLM-RoBERTa with QA Head (Start/End Pointers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c190b",
   "metadata": {
    "id": "335c190b"
   },
   "source": [
    "## Model 3: Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22e80db8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1759745905977,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "22e80db8",
    "outputId": "19c38ec1-aa37-41ea-9966-5f44e0cadc3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 (XLM-R QA) functions defined\n"
     ]
    }
   ],
   "source": [
    "def prepare_xlmr_qa_data(df, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Prepare data for XLM-R question answering (start/end positions).\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        question = row[\"question\"]\n",
    "        context = row[\"context\"]\n",
    "\n",
    "        # Tokenize with offsets\n",
    "        encoding = tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            max_length=max_length,\n",
    "            truncation=\"only_second\",  # Truncate context if needed\n",
    "            padding=False,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "\n",
    "        offset_mapping = encoding[\"offset_mapping\"]\n",
    "\n",
    "        # Find start and end positions\n",
    "        if row[\"answerable\"]:\n",
    "            answer_start_char = row[\"answer_start\"]\n",
    "            answer_text = row[\"answer\"]\n",
    "            answer_end_char = answer_start_char + len(answer_text)\n",
    "\n",
    "            # Find token positions\n",
    "            start_position = 0\n",
    "            end_position = 0\n",
    "\n",
    "            # Get context start in the encoding (after [CLS] question [SEP])\n",
    "            sequence_ids = encoding.sequence_ids()\n",
    "            context_start = sequence_ids.index(1)  # 1 means second sequence (context)\n",
    "\n",
    "            for i in range(context_start, len(offset_mapping)):\n",
    "                if sequence_ids[i] != 1:\n",
    "                    continue\n",
    "\n",
    "                token_start, token_end = offset_mapping[i]\n",
    "\n",
    "                # Check if this token overlaps with answer\n",
    "                if token_start <= answer_start_char < token_end:\n",
    "                    start_position = i\n",
    "\n",
    "                if token_start < answer_end_char <= token_end:\n",
    "                    end_position = i\n",
    "                    break\n",
    "\n",
    "            # If we didn't find valid positions, mark as unanswerable\n",
    "            if start_position == 0 and end_position == 0:\n",
    "                start_position = 0  # Point to [CLS] to indicate no answer\n",
    "                end_position = 0\n",
    "        else:\n",
    "            # Unanswerable: point to [CLS]\n",
    "            start_position = 0\n",
    "            end_position = 0\n",
    "\n",
    "        examples.append({\n",
    "            \"input_ids\": encoding[\"input_ids\"],\n",
    "            \"attention_mask\": encoding[\"attention_mask\"],\n",
    "            \"start_positions\": start_position,\n",
    "            \"end_positions\": end_position\n",
    "        })\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def compute_qa_metrics(start_logits, end_logits, start_positions, end_positions):\n",
    "    \"\"\"\n",
    "    Compute F1 for QA predictions.\n",
    "    Simplified: exact match of start/end positions.\n",
    "    \"\"\"\n",
    "    start_preds = np.argmax(start_logits, axis=-1)\n",
    "    end_preds = np.argmax(end_logits, axis=-1)\n",
    "\n",
    "    correct = 0\n",
    "    total = len(start_positions)\n",
    "\n",
    "    for start_pred, end_pred, start_true, end_true in zip(\n",
    "        start_preds, end_preds, start_positions, end_positions\n",
    "    ):\n",
    "        # Check if prediction matches ground truth\n",
    "        if start_pred == start_true and end_pred == end_true:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "    # Return as F1 proxy (in real scenario, would compute span overlap F1)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "print(\"Model 3 (XLM-R QA) functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9d2aa",
   "metadata": {
    "id": "79f9d2aa"
   },
   "source": [
    "## Model 3: Training & Evaluation\n",
    "\n",
    "Train a single multilingual XLM-RoBERTa QA model on all three languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "370c20b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6190,
     "status": "ok",
     "timestamp": 1759745912184,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "370c20b4",
    "outputId": "486545ff-cbe6-4682-ecde-40965468445c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MODEL 3: XLM-RoBERTa QA (Multilingual)\n",
      "======================================================================\n",
      "\n",
      "Preparing multilingual QA dataset...\n",
      "  Train: 3000 examples\n",
      "  Val: 900 examples\n",
      "\n",
      "Processing examples for QA format...\n",
      "  Train QA examples: 3000\n",
      "  Val QA examples: 900\n",
      "\n",
      "Initializing XLM-RoBERTa for Question Answering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model 3 ready for training\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MODEL 3: XLM-RoBERTa QA (Multilingual)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize tokenizer and model for QA\n",
    "model_name = \"xlm-roberta-base\"\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prepare multilingual QA dataset\n",
    "print(\"\\nPreparing multilingual QA dataset...\")\n",
    "df_train_qa = df_train.groupby(\"lang\").head(1000)  # 1000 per language\n",
    "df_val_qa = df_val.groupby(\"lang\").head(300)  # 300 per language\n",
    "\n",
    "print(f\"  Train: {len(df_train_qa)} examples\")\n",
    "print(f\"  Val: {len(df_val_qa)} examples\")\n",
    "\n",
    "print(\"\\nProcessing examples for QA format...\")\n",
    "train_qa_examples = prepare_xlmr_qa_data(df_train_qa, xlmr_tokenizer, max_length=384)\n",
    "val_qa_examples = prepare_xlmr_qa_data(df_val_qa, xlmr_tokenizer, max_length=384)\n",
    "\n",
    "print(f\"  Train QA examples: {len(train_qa_examples)}\")\n",
    "print(f\"  Val QA examples: {len(val_qa_examples)}\")\n",
    "\n",
    "# Convert to PyTorch Dataset with padding\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "def qa_collate_fn(batch):\n",
    "    \"\"\"Collate function to pad sequences in a batch\"\"\"\n",
    "    # Find max length in batch\n",
    "    max_len = max(len(ex['input_ids']) for ex in batch)\n",
    "\n",
    "    # Pad all sequences\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for ex in batch:\n",
    "        # Pad input_ids and attention_mask\n",
    "        pad_len = max_len - len(ex['input_ids'])\n",
    "        input_ids.append(ex['input_ids'] + [xlmr_tokenizer.pad_token_id] * pad_len)\n",
    "        attention_mask.append(ex['attention_mask'] + [0] * pad_len)\n",
    "        start_positions.append(ex['start_positions'])\n",
    "        end_positions.append(ex['end_positions'])\n",
    "\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "        'start_positions': torch.tensor(start_positions, dtype=torch.long),\n",
    "        'end_positions': torch.tensor(end_positions, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "train_qa_dataset = QADataset(train_qa_examples)\n",
    "val_qa_dataset = QADataset(val_qa_examples)\n",
    "\n",
    "# Initialize QA model\n",
    "print(\"\\nInitializing XLM-RoBERTa for Question Answering...\")\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "print(\"✓ Model 3 ready for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eca2427f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1759745912224,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "eca2427f",
    "outputId": "8b8ac598-ad3c-4e96-fe7f-c280eb90dcd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 training functions defined\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training function for QA model\n",
    "def train_qa_model(model, train_dataset, val_dataset, epochs=3, batch_size=8, lr=3e-5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=qa_collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=qa_collate_fn)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                start_positions=start_positions,\n",
    "                end_positions=end_positions\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1} - Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Prediction function for QA model\n",
    "def predict_qa_batch(model, dataset, batch_size=16):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=qa_collate_fn)\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc='Predicting'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Get start and end logits\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "\n",
    "            # Get best start and end positions\n",
    "            start_preds = torch.argmax(start_logits, dim=1).cpu().numpy()\n",
    "            end_preds = torch.argmax(end_logits, dim=1).cpu().numpy()\n",
    "\n",
    "            for i in range(len(start_preds)):\n",
    "                start_pos = start_preds[i]\n",
    "                end_pos = end_preds[i]\n",
    "\n",
    "                # Ensure valid span (end >= start)\n",
    "                if end_pos < start_pos:\n",
    "                    end_pos = start_pos\n",
    "\n",
    "                all_predictions.append((start_pos, end_pos))\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "print(\"Model 3 training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "278936f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 422689,
     "status": "ok",
     "timestamp": 1759746458015,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "278936f7",
    "outputId": "a3ba742d-308a-4a47-fa58-2222e4e1b06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING MODEL 3\n",
      "======================================================================\n",
      "Training XLM-RoBERTa QA model on all languages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 375/375 [03:00<00:00,  2.08it/s, loss=2.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Average Loss: 2.9884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 375/375 [03:00<00:00,  2.08it/s, loss=2.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Average Loss: 2.0602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 375/375 [03:00<00:00,  2.08it/s, loss=2.36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Average Loss: 1.3530\n",
      "\n",
      "✓ Model 3 training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train multilingual QA model\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING MODEL 3\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Training XLM-RoBERTa QA model on all languages...\")\n",
    "qa_model_multi = train_qa_model(qa_model, train_qa_dataset, val_qa_dataset, epochs=3, batch_size=8)\n",
    "print(\"\\n✓ Model 3 training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ae18f62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16576,
     "status": "ok",
     "timestamp": 1759746474597,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "6ae18f62",
    "outputId": "726c31ee-b92c-4a0f-d77c-9647a9f7d3d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL 3 RESULTS PER LANGUAGE\n",
      "======================================================================\n",
      "\n",
      "==================================================\n",
      "Evaluating Arabic...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 19/19 [00:04<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Exact Match: 0.3967\n",
      "  F1 Score: 0.4890\n",
      "  Samples: 300\n",
      "\n",
      "  Example Predictions:\n",
      "\n",
      "  Example 1:\n",
      "    Question: ما هي أولى جامعات فنلندا؟...\n",
      "    Predicted: 'Royal Academy of Åbo'\n",
      "    Gold: 'Royal Academy of Åbo'\n",
      "\n",
      "  Example 2:\n",
      "    Question: ما عدد الدول المطلة على بحر البلطيق؟...\n",
      "    Predicted: 'Finland, Sweden'\n",
      "    Gold: 'Finland, Sweden, Denmark, Estonia, Latvia, Lithuania, northwest Russia, Poland, Germany'\n",
      "\n",
      "  Example 3:\n",
      "    Question: اين عاش نيوتن؟...\n",
      "    Predicted: 'hat'\n",
      "    Gold: 'Grantham'\n",
      "\n",
      "==================================================\n",
      "Evaluating Korean...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 19/19 [00:04<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Exact Match: 0.4600\n",
      "  F1 Score: 0.5532\n",
      "  Samples: 300\n",
      "\n",
      "  Example Predictions:\n",
      "\n",
      "  Example 1:\n",
      "    Question: 북유럽의 노르딕 국가는 몇개인가요?...\n",
      "    Predicted: '12 million'\n",
      "    Gold: 'five'\n",
      "\n",
      "  Example 2:\n",
      "    Question: 1887년 케이스 웨스턴 리저브 대학의 이름은 무엇인가?...\n",
      "    Predicted: '1967'\n",
      "    Gold: 'Western Reserve University (formerly Western Reserve College) and Case Institute of Technology (formerly Case School of Applied Science)'\n",
      "\n",
      "  Example 3:\n",
      "    Question: 옴진리교는 어느 나라에서 시작된 종교인가?...\n",
      "    Predicted: 'Egyptian'\n",
      "    Gold: 'Egypt'\n",
      "\n",
      "==================================================\n",
      "Evaluating Telugu...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 19/19 [00:04<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Exact Match: 0.3933\n",
      "  F1 Score: 0.5026\n",
      "  Samples: 300\n",
      "\n",
      "  Example Predictions:\n",
      "\n",
      "  Example 1:\n",
      "    Question: ఒరెగాన్ రాష్ట్రంలోని అతిపెద్ద నగరం ఏది ?...\n",
      "    Predicted: 'Portland'\n",
      "    Gold: 'Portland'\n",
      "\n",
      "  Example 2:\n",
      "    Question: కలరా వ్యాధిని మొదటగా ఏ దేశంలో కనుగొన్నారు ?...\n",
      "    Predicted: 'Indonesia'\n",
      "    Gold: 'Indian subcontinent'\n",
      "\n",
      "  Example 3:\n",
      "    Question: కలరా వ్యాధిని మొదటగా ఏ దేశంలో కనుగొన్నారు ?...\n",
      "    Predicted: 'United States'\n",
      "    Gold: 'England'\n",
      "\n",
      "======================================================================\n",
      "MODEL 3 RESULTS SUMMARY\n",
      "======================================================================\n",
      "   model language lang_code  f1_score  exact_match\n",
      "XLM-R QA   Arabic        ar  0.489022     0.396667\n",
      "XLM-R QA   Korean        ko  0.553180     0.460000\n",
      "XLM-R QA   Telugu        te  0.502644     0.393333\n"
     ]
    }
   ],
   "source": [
    "# Evaluate QA model per language\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 3 RESULTS PER LANGUAGE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model3_results = []\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Evaluating {LANGUAGE_NAMES[lang]}...\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    # Get language-specific validation data\n",
    "    df_val_lang = df_val[df_val[\"lang\"] == lang].head(300)\n",
    "\n",
    "    # Prepare QA examples\n",
    "    val_lang_qa = prepare_xlmr_qa_data(df_val_lang, xlmr_tokenizer, max_length=384)\n",
    "    val_lang_dataset = QADataset(val_lang_qa)\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = predict_qa_batch(qa_model_multi, val_lang_dataset, batch_size=16)\n",
    "\n",
    "    # Decode predictions to text\n",
    "    pred_spans = []\n",
    "    gold_spans = []\n",
    "\n",
    "    for i, (start_pos, end_pos) in enumerate(predictions):\n",
    "        row = df_val_lang.iloc[i]\n",
    "\n",
    "        # Get the encoding for this example\n",
    "        encoding = xlmr_tokenizer(\n",
    "            row[\"question\"],\n",
    "            row[\"context\"],\n",
    "            max_length=384,\n",
    "            truncation=\"only_second\",\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "\n",
    "        # Check if unanswerable\n",
    "        if start_pos == 0 or end_pos == 0:\n",
    "            pred_spans.append(\"\")\n",
    "        else:\n",
    "            # Decode the predicted span\n",
    "            input_ids = encoding[\"input_ids\"]\n",
    "            answer_tokens = input_ids[start_pos:end_pos+1]\n",
    "            pred_text = xlmr_tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "            pred_spans.append(pred_text.strip())\n",
    "\n",
    "        # Gold answer\n",
    "        if row[\"answerable\"]:\n",
    "            gold_spans.append(row[\"answer\"])\n",
    "        else:\n",
    "            gold_spans.append(\"\")\n",
    "\n",
    "    # Calculate exact match\n",
    "    exact_matches = sum(1 for pred, gold in zip(pred_spans, gold_spans) if pred.strip() == gold.strip())\n",
    "    em_score = exact_matches / len(gold_spans) if len(gold_spans) > 0 else 0.0\n",
    "\n",
    "    # Calculate F1 (token-level overlap)\n",
    "    f1_scores = []\n",
    "    for pred, gold in zip(pred_spans, gold_spans):\n",
    "        pred_tokens = set(pred.split())\n",
    "        gold_tokens = set(gold.split())\n",
    "\n",
    "        if len(pred_tokens) == 0 and len(gold_tokens) == 0:\n",
    "            f1_scores.append(1.0)\n",
    "        elif len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
    "            f1_scores.append(0.0)\n",
    "        else:\n",
    "            overlap = len(pred_tokens & gold_tokens)\n",
    "            precision = overlap / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
    "            recall = overlap / len(gold_tokens) if len(gold_tokens) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "    avg_f1 = sum(f1_scores) / len(f1_scores) if len(f1_scores) > 0 else 0.0\n",
    "\n",
    "    model3_results.append({\n",
    "        \"model\": \"XLM-R QA\",\n",
    "        \"language\": LANGUAGE_NAMES[lang],\n",
    "        \"lang_code\": lang,\n",
    "        \"f1_score\": avg_f1,\n",
    "        \"exact_match\": em_score\n",
    "    })\n",
    "\n",
    "    print(f\"  Exact Match: {em_score:.4f}\")\n",
    "    print(f\"  F1 Score: {avg_f1:.4f}\")\n",
    "    print(f\"  Samples: {len(gold_spans)}\")\n",
    "\n",
    "    # Show some examples\n",
    "    print(\"\\n  Example Predictions:\")\n",
    "    for i in range(min(3, len(pred_spans))):\n",
    "        print(f\"\\n  Example {i+1}:\")\n",
    "        print(f\"    Question: {df_val_lang.iloc[i]['question'][:80]}...\")\n",
    "        print(f\"    Predicted: '{pred_spans[i]}'\")\n",
    "        print(f\"    Gold: '{gold_spans[i]}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL 3 RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "results_df = pd.DataFrame(model3_results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac19db8d",
   "metadata": {
    "id": "ac19db8d"
   },
   "source": [
    "# Final Model Comparison\n",
    "\n",
    "This section provides a comprehensive comparison of all three models across all languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd59a7a",
   "metadata": {
    "id": "9cd59a7a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compile all results into a comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "# Model 1: BiLSTM-CRF (per-language models)\n",
    "# model1_results is a list of dicts with keys: model, language, lang_code, f1_score\n",
    "for result in model1_results:\n",
    "    comparison_data.append({\n",
    "        'Model': 'BiLSTM-CRF',\n",
    "        'Language': result['language'],\n",
    "        'F1 Score': result['f1_score'],\n",
    "        'Exact Match': 0,  # Model 1 doesn't compute exact match\n",
    "        'Training': 'Per-language',\n",
    "        'Labeling': 'BIO'\n",
    "    })\n",
    "\n",
    "# Model 2: XLM-R Token Classification (multilingual)\n",
    "# model2_results is a list of dicts with keys: model, language, lang_code, f1_score\n",
    "for result in model2_results:\n",
    "    comparison_data.append({\n",
    "        'Model': 'XLM-R Token',\n",
    "        'Language': result['language'],\n",
    "        'F1 Score': result['f1_score'],\n",
    "        'Exact Match': 0,  # Model 2 doesn't compute exact match\n",
    "        'Training': 'Multilingual',\n",
    "        'Labeling': 'BIO'\n",
    "    })\n",
    "\n",
    "# Model 3: XLM-R QA (multilingual)\n",
    "# model3_results is a list of dicts with keys: model, language, lang_code, f1_score, exact_match\n",
    "for result in model3_results:\n",
    "    comparison_data.append({\n",
    "        'Model': 'XLM-R QA',\n",
    "        'Language': result['language'],\n",
    "        'F1 Score': result['f1_score'],\n",
    "        'Exact Match': result['exact_match'],\n",
    "        'Training': 'Multilingual',\n",
    "        'Labeling': 'Start/End'\n",
    "    })\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "417904c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "executionInfo": {
     "elapsed": 685,
     "status": "ok",
     "timestamp": 1759746475319,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "417904c6",
    "outputId": "1b7cb053-af5c-45a1-f361-59605dcd529a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize F1 Score comparison\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plot 1: F1 scores by model and language\u001b[39;00m\n\u001b[0;32m      5\u001b[0m pivot_f1 \u001b[38;5;241m=\u001b[39m comparison_df\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLanguage\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize F1 Score comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: F1 scores by model and language\n",
    "pivot_f1 = comparison_df.pivot(index='Language', columns='Model', values='F1 Score')\n",
    "pivot_f1.plot(kind='bar', ax=axes[0], rot=0, width=0.8)\n",
    "axes[0].set_title('F1 Score Comparison Across Models and Languages', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Language', fontsize=12)\n",
    "axes[0].set_ylabel('F1 Score', fontsize=12)\n",
    "axes[0].legend(title='Model', loc='best')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Exact Match by model and language\n",
    "pivot_em = comparison_df.pivot(index='Language', columns='Model', values='Exact Match')\n",
    "pivot_em.plot(kind='bar', ax=axes[1], rot=0, width=0.8, color=['#ff9999', '#66b3ff', '#99ff99'])\n",
    "axes[1].set_title('Exact Match Comparison Across Models and Languages', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Language', fontsize=12)\n",
    "axes[1].set_ylabel('Exact Match Score', fontsize=12)\n",
    "axes[1].legend(title='Model', loc='best')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Average performance by model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AVERAGE PERFORMANCE BY MODEL\")\n",
    "print(\"=\"*60)\n",
    "avg_by_model = comparison_df.groupby('Model')[['F1 Score', 'Exact Match']].mean()\n",
    "print(avg_by_model)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Average performance by language\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AVERAGE PERFORMANCE BY LANGUAGE\")\n",
    "print(\"=\"*60)\n",
    "avg_by_lang = comparison_df.groupby('Language')[['F1 Score', 'Exact Match']].mean()\n",
    "print(avg_by_lang)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5a23e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1759746475365,
     "user": {
      "displayName": "Mateus Spencer",
      "userId": "04318993233549825296"
     },
     "user_tz": -60
    },
    "id": "2aa5a23e",
    "outputId": "4fccce40-2306-4075-f285-b565f1eb571a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Save results for future reference\n",
    "import json\n",
    "\n",
    "results_summary = {\n",
    "    'model_1_bilstm_crf': model1_results,\n",
    "    'model_2_xlmr_token': model2_results,\n",
    "    'model_3_xlmr_qa': model3_results,\n",
    "    'comparison_table': comparison_df.to_dict('records')\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open('week39_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "023c020b383746708938c4e13e7481a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0452210a534342c29f6c11631bbbff96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04a5b758993f4484b5169e74416ccafe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "055d5dabbe89442ca110bef40ac7f0e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "087c77bd7fa841a79458184d63b5cfd0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d4c1075642043caa476c20fa34f26b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d372cc8506f24070a0cb68fbb23b2555",
      "placeholder": "​",
      "style": "IPY_MODEL_c508810aad174ca99ffb584e8df7b141",
      "value": "model.safetensors: 100%"
     }
    },
    "14180c3d58a04b9d9cd3792ff495d1ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f7d24468b52471c91ceed7ae5198e7e",
      "max": 25,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c20385491c824aadbca4fc60517dec67",
      "value": 25
     }
    },
    "15c0d220282f4fde93b9d3ffb73c3c43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3cd8eb1290041d1ae0580417e0811dc",
      "max": 9096718,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04a5b758993f4484b5169e74416ccafe",
      "value": 9096718
     }
    },
    "1805a03b8d42455dae13568f8f4ba968": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e65c11b91a6437c9c2b724eef65a2a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1fc5343eea6744fda15cf6101aca2787": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "281f2ceafceb467c8a91be949a47e4cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46001449c0644e9e8add4fa260237881": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7e5e3445134549069c8f80ba60be11a8",
       "IPY_MODEL_8c27937eff72449fb9e88b5a3e5d3752",
       "IPY_MODEL_b55718da47f74cdba5ffe9b4c04c9976"
      ],
      "layout": "IPY_MODEL_0452210a534342c29f6c11631bbbff96"
     }
    },
    "4f0ec89bfb4c46799064c5c06416d101": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "50a2c26a4e5c4ee1bea16357e0fefe00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "567dcd809dd041c191fbd4e6ae9122d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e59589a16f3947e3984de464e7f9483a",
       "IPY_MODEL_14180c3d58a04b9d9cd3792ff495d1ad",
       "IPY_MODEL_5a38c4bf49b7445586bd3d48ad0a379e"
      ],
      "layout": "IPY_MODEL_89f7232ba4ae4a2a93201938f82b1294"
     }
    },
    "5a38c4bf49b7445586bd3d48ad0a379e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9d82322ecd643eaa1fa1f02e693c6ee",
      "placeholder": "​",
      "style": "IPY_MODEL_4f0ec89bfb4c46799064c5c06416d101",
      "value": " 25.0/25.0 [00:00&lt;00:00, 864B/s]"
     }
    },
    "5aff37da77234fcc9597b213c415e00f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "61b356d80166452e983f26e1f9b1e314": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fc9c7dd7af74531b3ef71f94e9994eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a242866c369840e1a31848b00b2634ff",
      "placeholder": "​",
      "style": "IPY_MODEL_8277eadd11e5476bbcc21ea2061c6be3",
      "value": " 1.12G/1.12G [00:30&lt;00:00, 54.1MB/s]"
     }
    },
    "7e5e3445134549069c8f80ba60be11a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_023c020b383746708938c4e13e7481a7",
      "placeholder": "​",
      "style": "IPY_MODEL_b4aec88a16034d9c9fbca16306dba555",
      "value": "config.json: 100%"
     }
    },
    "8277eadd11e5476bbcc21ea2061c6be3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "862b9dfa4cf54547b28727f16fdc7487": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89f7232ba4ae4a2a93201938f82b1294": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a103e1285774bb0af5b2dbfc5d44be9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c27937eff72449fb9e88b5a3e5d3752": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a103e1285774bb0af5b2dbfc5d44be9",
      "max": 615,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bafe703ef70046d5848e648c5709c76d",
      "value": 615
     }
    },
    "8f7d24468b52471c91ceed7ae5198e7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97e2c31ec3c240c5bb3be0f2618178b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_862b9dfa4cf54547b28727f16fdc7487",
      "placeholder": "​",
      "style": "IPY_MODEL_9af4ee1e39424752bc3a0c5362eaa5aa",
      "value": " 5.07M/5.07M [00:00&lt;00:00, 13.4MB/s]"
     }
    },
    "9af4ee1e39424752bc3a0c5362eaa5aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9df74f128f294ff9b188eddcf822ee0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f292e9bea00d47e3be0860e9a67a724d",
       "IPY_MODEL_abee68ae86b4408eb188dcba72b88a27",
       "IPY_MODEL_97e2c31ec3c240c5bb3be0f2618178b4"
      ],
      "layout": "IPY_MODEL_a40ab5efbb304784b14e04841617ddad"
     }
    },
    "a1843cfb9ade4663986a91df33bc7a62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a242866c369840e1a31848b00b2634ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a40ab5efbb304784b14e04841617ddad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6c064c775c74fd4b7aff2ae0b08c134": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abee68ae86b4408eb188dcba72b88a27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_281f2ceafceb467c8a91be949a47e4cd",
      "max": 5069051,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5aff37da77234fcc9597b213c415e00f",
      "value": 5069051
     }
    },
    "ac93dafb474b4ce0b2507836835b3710": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ae0adce1fc0646bcbc71ac4f4b074fbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0f267aa1a8f4be28d18dc102c8df2ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0d4c1075642043caa476c20fa34f26b4",
       "IPY_MODEL_df238a6c4cec4dbda81b9196d49263f5",
       "IPY_MODEL_6fc9c7dd7af74531b3ef71f94e9994eb"
      ],
      "layout": "IPY_MODEL_a6c064c775c74fd4b7aff2ae0b08c134"
     }
    },
    "b3cd8eb1290041d1ae0580417e0811dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4aec88a16034d9c9fbca16306dba555": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b55718da47f74cdba5ffe9b4c04c9976": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61b356d80166452e983f26e1f9b1e314",
      "placeholder": "​",
      "style": "IPY_MODEL_1e65c11b91a6437c9c2b724eef65a2a0",
      "value": " 615/615 [00:00&lt;00:00, 21.3kB/s]"
     }
    },
    "b9d82322ecd643eaa1fa1f02e693c6ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bafe703ef70046d5848e648c5709c76d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c20385491c824aadbca4fc60517dec67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c508810aad174ca99ffb584e8df7b141": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c8a766f4a889476b914a416ada7854ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dc54ee016a174f889d7c35fca5590165",
       "IPY_MODEL_15c0d220282f4fde93b9d3ffb73c3c43",
       "IPY_MODEL_f1bd603ad1bd411889a8c7a23cbf5f9e"
      ],
      "layout": "IPY_MODEL_d0f9c334f7274949af75ff45f5ba94c6"
     }
    },
    "d0c41d79a56743a798b5352671e2a79f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0f9c334f7274949af75ff45f5ba94c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d372cc8506f24070a0cb68fbb23b2555": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db44c326850d4bbeaf95d66e7276f9c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc54ee016a174f889d7c35fca5590165": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_055d5dabbe89442ca110bef40ac7f0e8",
      "placeholder": "​",
      "style": "IPY_MODEL_50a2c26a4e5c4ee1bea16357e0fefe00",
      "value": "tokenizer.json: 100%"
     }
    },
    "df238a6c4cec4dbda81b9196d49263f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae0adce1fc0646bcbc71ac4f4b074fbf",
      "max": 1115567652,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ac93dafb474b4ce0b2507836835b3710",
      "value": 1115567652
     }
    },
    "e59589a16f3947e3984de464e7f9483a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1805a03b8d42455dae13568f8f4ba968",
      "placeholder": "​",
      "style": "IPY_MODEL_db44c326850d4bbeaf95d66e7276f9c4",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "f1bd603ad1bd411889a8c7a23cbf5f9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_087c77bd7fa841a79458184d63b5cfd0",
      "placeholder": "​",
      "style": "IPY_MODEL_a1843cfb9ade4663986a91df33bc7a62",
      "value": " 9.10M/9.10M [00:00&lt;00:00, 38.2MB/s]"
     }
    },
    "f292e9bea00d47e3be0860e9a67a724d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fc5343eea6744fda15cf6101aca2787",
      "placeholder": "​",
      "style": "IPY_MODEL_d0c41d79a56743a798b5352671e2a79f",
      "value": "sentencepiece.bpe.model: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
